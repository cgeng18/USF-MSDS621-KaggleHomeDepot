{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mt = pd.read_csv('modified_train.csv')\n",
    "mt = mt.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    stops = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if (len(w) > 2 and (w not in stops))]  # ignore a, an, to, at, be, ...\n",
    "    # print words\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def stemwords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list with each word\n",
    "    stemmed using a PorterStemmer.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(t) for t in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def tokenizer(text):\n",
    "    return stemwords(tokenize(text))\n",
    "\n",
    "\n",
    "def attrib_stack(attributes):\n",
    "    attributes['value'] = attributes['value'].apply(lambda x: str(x))\n",
    "    attrib_per_product = attributes.groupby('product_uid').agg(lambda x: x.tolist())\n",
    "    attrib_per_product = attrib_per_product.reset_index()\n",
    "    attrib_per_product['value'] = attrib_per_product['value'].apply(lambda x: ','.join(x))\n",
    "    attrib_per_product['value'] = attrib_per_product['value'].apply(lambda x: tokenizer(x))\n",
    "    attrib_per_product['value'] = attrib_per_product['value'].apply(lambda x: ','.join(x))\n",
    "    attrib_per_product.to_csv('attrib_per_product.csv')\n",
    "    attrib_per_product = pd.read_csv('attrib_per_product.csv')\n",
    "    attrib_per_product = attrib_per_product.drop('Unnamed: 0' ,axis = 1)\n",
    "    return attrib_per_product\n",
    "\n",
    "def join_attrib(train, attrib_per_product):\n",
    "    train = train.set_index('product_uid').join(attrib_per_product.set_index('product_uid'))\n",
    "    train = train.reset_index()\n",
    "    attrib_per_product = attrib_per_product.reset_index()\n",
    "    return train, attrib_per_product\n",
    "    \n",
    "def search_term_in_attrib(train):\n",
    "    train['value'].fillna('', inplace = True)\n",
    "    train['value'] = train['value'].apply(lambda x: set(x.split(',')))\n",
    "    train['search_term_split'] = train['search_term'].apply(lambda x: set(tokenizer(x)))\n",
    "    search_term_in_attrib = [] \n",
    "    for i in range(74067):\n",
    "        p = len(train['search_term_split'][i].intersection(train['value'][i]))\n",
    "        search_term_in_attrib.append(p)\n",
    "    train['search_term_in_attrib'] = search_term_in_attrib\n",
    "    return train\n",
    "\n",
    "def color_df(attributes, train):\n",
    "    attrib_col = attributes[attributes['name'].apply(lambda x: 'color' in str(x).lower())]\n",
    "    attrib_col = attrib_col.groupby('product_uid').agg(lambda x: x.tolist())\n",
    "    attrib_col = attrib_col.drop('name',axis = 1)\n",
    "    attrib_col = attrib_col.reset_index()\n",
    "    attrib_col = attrib_col.rename(columns={'value': 'color'})\n",
    "    attrib_col['color'] = attrib_col['color'].apply(lambda x: ','.join(x))\n",
    "    attrib_col['color'] = attrib_col['color'].apply(lambda x: ','.join(x.replace('/','').replace(' ',',').split(',')).replace(',,',','))\n",
    "    train = train.set_index('product_uid').join(attrib_col.set_index('product_uid'))\n",
    "    train = train.reset_index()\n",
    "    attrib_col = attrib_col.reset_index()\n",
    "    train['color'].fillna('', inplace = True)\n",
    "    train['search_term'].fillna('', inplace = True)\n",
    "    train['color'] = train['color'].apply(lambda x: set(x.split(',')))\n",
    "    color_in_search_term = [] \n",
    "    for i in range(74067):\n",
    "        p = len(train['color'][i].intersection(train['search_term_split'][i]))\n",
    "        color_in_search_term.append(p)\n",
    "    train['color_in_search_term']= color_in_search_term\n",
    "    \n",
    "    return train\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('attributes.csv', encoding='ISO-8859-1')\n",
    "attrib_per_product = attrib_stack(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>['Bullet01', 'Bullet02', 'Bullet03', 'Bullet04...</td>\n",
       "      <td>versatil,connector,variou,connect,home,repair,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002.0</td>\n",
       "      <td>['Application Method', 'Assembled Depth (in.)'...</td>\n",
       "      <td>brush,roller,spray,reviv,wood,composit,deck,ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                               name  \\\n",
       "0     100001.0  ['Bullet01', 'Bullet02', 'Bullet03', 'Bullet04...   \n",
       "1     100002.0  ['Application Method', 'Assembled Depth (in.)'...   \n",
       "\n",
       "                                               value  \n",
       "0  versatil,connector,variou,connect,home,repair,...  \n",
       "1  brush,roller,spray,reviv,wood,composit,deck,ra...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrib_per_product.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
    "train, attrib_per_product = join_attrib(train, attrib_per_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = search_term_in_attrib(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74067"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = color_df(attributes, train)\n",
    "train = search_title_lev_dist(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_title_lev_dist(train):\n",
    "    from Levenshtein import distance\n",
    "    train.to_csv('train_with_search_in_attrib.csv')\n",
    "    train = pd.read_csv('train_with_search_in_attrib.csv')\n",
    "    train = train.drop(['Unnamed: 0'], axis = 1)\n",
    "    train['product_title_clean'] = train['product_title'].apply(lambda x: list(set(tokenize(x))))\n",
    "    train['search_term'].fillna('', inplace = True)\n",
    "    train['search_term_split'] = train['search_term'].apply(lambda x: x.split(' '))\n",
    "    \n",
    "    p = []\n",
    "    for i in range(0,74067):\n",
    "        q = []\n",
    "        if len(train['search_term_split'][i][0])>0:\n",
    "            for j in range(len(train['search_term_split'][i])):\n",
    "                for k in range(len(train['product_title_clean'][i])):\n",
    "                    print(i,j)\n",
    "                    if train['search_term_split'][i][j] in train['product_title_clean'][i][k]:\n",
    "                        q.append((train['product_title_clean'][i][k],train['product_title_clean'][i][k]))\n",
    "                        continue\n",
    "                    elif train['search_term_split'][i][j][0] == train['product_title_clean'][i][k][0]:\n",
    "                        q.append((train['search_term_split'][i][j], train['product_title_clean'][i][k]))\n",
    "        p.append(q)\n",
    "    \n",
    "    l = []\n",
    "    for i in range(len(p)):\n",
    "        q = []\n",
    "        for j in range(len(p[i])):\n",
    "            q.append(distance(p[i][j][0], p[i][j][1]))\n",
    "        l.append(q)\n",
    "    m = []\n",
    "    for q in l:\n",
    "        if q == []:\n",
    "            m.append(1000)\n",
    "        else :\n",
    "            m.append(min(q))\n",
    "\n",
    "    train['min_levenstein_dist_title'] = m\n",
    "    \n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_with_search_in_attrib.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_brand_lev_dist(train, attributes):\n",
    "    from collections import defaultdict\n",
    "    from Levenshtein import distance\n",
    "    attr_brand = attributes[(attributes['name'].str.lower().str.contains('brand')==True) & attributes['value'].notnull()]\n",
    "    attr_brand = attr_brand.drop('name',axis=1)\n",
    "    attr_brand =attr_brand.rename(columns = {'value':'brand'})\n",
    "    attr_brand['product_uid'] = attr_brand['product_uid'].apply(lambda x:int(x))\n",
    "    \n",
    "    d = defaultdict(list)\n",
    "    p =list(attr_brand['product_uid'])\n",
    "    b= list(attr_brand['brand'])\n",
    "    for i in range(len(l)):\n",
    "        if l[i] not in d:\n",
    "            d[l[i]] = tokenize(b[i])\n",
    "        else:\n",
    "            continue\n",
    "    train['brand'] = train['product_uid'].apply(lambda x: d[x])\n",
    "    train['brand'].fillna('',inplace=True)\n",
    "    train['search_term'].fillna('', inplace = True)\n",
    "    train['search_term_split'] =  train['search_term'].apply(lambda x: x.split(' '))\n",
    "    \n",
    "    p = []\n",
    "    for i in range(74067):\n",
    "        q = []\n",
    "        if len(train['search_term_split'][i][0])>0:\n",
    "            for j in range(len(train['search_term_split'][i])):\n",
    "                for k in range(len(train['brand'][i])):\n",
    "                    print(i,j)\n",
    "                    if train['search_term_split'][i][j] in train['brand'][i][k]:\n",
    "                        q.append((train['brand'][i][k],train['brand'][i][k]))\n",
    "                        continue\n",
    "                    elif train['search_term_split'][i][j][0] == train['brand'][i][k][0]:\n",
    "                        q.append((train['search_term_split'][i][j], train['brand'][i][k]))\n",
    "        p.append(q)\n",
    "    l = []\n",
    "    for i in range(len(p)):\n",
    "        q = []\n",
    "        for j in range(len(p[i])):\n",
    "            q.append(distance(p[i][j][0], p[i][j][1]))\n",
    "        l.append(q)\n",
    "    m = []\n",
    "    for q in l:\n",
    "        if q == []:\n",
    "            m.append(1000)\n",
    "        else :\n",
    "            m.append(min(q))\n",
    "            \n",
    "    train['min_levenstein_dist_brand'] = m\n",
    "    \n",
    "    return train\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = search_brand_lev_dist(train, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_with_search_in_attrib.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
