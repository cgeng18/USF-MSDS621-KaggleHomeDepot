{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add three new features onto existing dataframe, which is Jaro distance,Normalized compression distance, and Smith-Waterman algorithm to compute similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'product_descriptions.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-488665abcca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mMODIFIED_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"modified_train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDESC_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf_train_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_uid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_uid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'product_descriptions.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Get data to work on. Join the table product_descriptions.csv and train.\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "DESC_FILE = \"product_descriptions.csv\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "MODIFIED_FILE = \"modified_train.csv\"\n",
    "\n",
    "df_desc = pd.read_csv(DESC_FILE)\n",
    "df_train = pd.read_csv(TRAIN_FILE,encoding=\"latin-1\")\n",
    "df_train_desc = df_train.set_index('product_uid').join(df_desc.set_index('product_uid'))\n",
    "df_modified_train = pd.read_csv(MODIFIED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add clean description for computing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_desc = pd.read_csv(\"clean_desc.csv\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to add clean description to df_train_desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "tryout = df_train.iloc[:20]\n",
    "ap = clean_desc.iloc[:20]\n",
    "merged = tryout.assign(clean_desc = ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_desc = df_train_desc.assign(clean_desc=clean_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the search_terms, description, titles for computing three metrics.\n",
    "values = {'product_descption': \"\", 'product_title': \"\", 'search_term': \"\", 'relevance': \"\"}\n",
    "df_train_desc = df_train_desc.fillna(value=values)\n",
    "\n",
    "desc_col = df_train_desc['product_description']\n",
    "search_col = df_train_desc[\"search_term\"]\n",
    "title_col = df_train_desc[\"product_title\"]\n",
    "clean_term_col = df_modified_train[\"cleaned_terms\"]\n",
    "terms_neighbors_col = df_modified_train[\"terms_neighbour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: \n",
    "1. develop prototype of three metrics computation.\n",
    "2. feed on clean description and original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Jaro distance on two pd.series that represents text columns.\n",
    "https://rosettacode.org/wiki/Jaro_distance#Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro(s, t):\n",
    "    s_len = len(s)\n",
    "    t_len = len(t)\n",
    "\n",
    "    if s_len == 0 and t_len == 0:\n",
    "        return 1\n",
    "\n",
    "    match_distance = (max(s_len, t_len) // 2) - 1\n",
    "\n",
    "    s_matches = [False] * s_len\n",
    "    t_matches = [False] * t_len\n",
    "\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(s_len):\n",
    "        start = max(0, i-match_distance)\n",
    "        end = min(i+match_distance+1, t_len)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if t_matches[j]:\n",
    "                continue\n",
    "            if s[i] != t[j]:\n",
    "                continue\n",
    "            s_matches[i] = True\n",
    "            t_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "    if matches == 0:\n",
    "        return 0\n",
    "    k = 0\n",
    "    for i in range(s_len):\n",
    "        if not s_matches[i]:\n",
    "            continue\n",
    "        while not t_matches[k]:\n",
    "            k += 1\n",
    "        if s[i] != t[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "    return ((matches / s_len) +\n",
    "            (matches / t_len) +\n",
    "            ((matches - transpositions/2) / matches)) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: to do the computation of two documents' similarity score. I can think of using each search terms and find the average(? could be weighted for terms that appeared more frequently) of Jaro distiance with respect to each terms in descriptions.  \n",
    "The using of Jaro score require select a threshold. Jaro describe the editing distance, for example soup and sour has score as 0.8333, but they are definite not the same thing. 0.8 as a threshold should be sufficient. In the context of product description, we can rule out such random similar words' appearance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJaroScoreOnDocs(query, long_text):\n",
    "    #transform query and long_text to list of words.\n",
    "    query_ls = query.split()\n",
    "    long_text_ls = long_text.split()\n",
    "    \n",
    "    total_J_score = 0\n",
    "    for i in query_ls:\n",
    "        j_score_in_i = sum([jaro(i,j) for j in long_text_ls if jaro(i,j)>0.83])\n",
    "        total_J_score += j_score_in_i\n",
    "    return total_J_score\n",
    "#desc_col.iloc[0]\n",
    "#terms_neighbors_col.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the total jaro distance w.r.t each search terms and description in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJaroCol(df,query_col_name,text_col_name,new_col_name):\n",
    "    # Could combine title and description as a unit to compute Jaro score. It will be higher but as one score, easy to compute.\n",
    "    # compute all jscore in a list\n",
    "    j_score_ls = []\n",
    "    for i in range(len(df)):\n",
    "        query = df[query_col_name].iloc[i]\n",
    "        long_text = df[text_col_name].iloc[i]\n",
    "        j_score = getJaroScoreOnDocs(query,long_text)\n",
    "        j_score_ls.append(j_score)\n",
    "    df[new_col_name] = j_score_ls\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes about 3mins to create a jaro score on query and description.\n",
    "createJaroCol(df_train_desc,\"search_term\",\"product_description\",\"jscore_query_desc\")\n",
    "createJaroCol(df_train_desc,\"search_term\",\"product_title\",\"jscore_query_title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result in df_train_desc.df_train_desc.headad()\n",
    "# Prepare for writing csv file for two new features.\n",
    "output1 = df_train_desc.drop([\"product_title\",\"search_term\",\"relevance\",\"product_description\"],axis=1)\n",
    "# Make csv file.\n",
    "output1.to_csv(\"two_jscore_feature.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Smith–Waterman algorithm to compute string similarity\n",
    "sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional.\n",
    "The SW algorithm will yield a matrix represents all possible alignment's score. We can use two string to get the optimal alignment sequence and compute the alignment score. For all the combination between search terms and description, we can collect the overall optimal alignment sequence score. For example, screw and driver \n",
    "https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm#Substitution_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smith_waterman(a: str, b: str, alignment_score: float = 1, gap_cost: float = 1) -> float:\n",
    "    \"\"\"\n",
    "  Compute the Smith-Waterman alignment score for two strings.\n",
    "  See https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm#Algorithm\n",
    "  This implementation has a fixed gap cost (i.e. extending a gap is considered\n",
    "  free). In the terminology of the Wikipedia description, W_k = {c, c, c, ...}.\n",
    "  This implementation also has a fixed alignment score, awarded if the relevant\n",
    "  characters are equal.\n",
    "  Kinda slow, especially for large (50+ char) inputs.\n",
    "    \"\"\"\n",
    "# H holds the alignment score at each point, computed incrementally\n",
    "    H = np.zeros((len(a) + 1, len(b) + 1))\n",
    "    for i in range(1, len(a) + 1):\n",
    "        for j in range(1, len(b) + 1):\n",
    "      # The score for substituting the letter a[i-1] for b[j-1]. Generally low\n",
    "      # for mismatch, high for match.\n",
    "            match = H[i-1,j-1] + (alignment_score if a[i-1] == b[j-1] else 0)\n",
    "      # The scores for for introducing extra letters in one of the strings (or\n",
    "      # by symmetry, deleting them from the other).\n",
    "            delete = H[1:i,j].max() - gap_cost if i > 1 else 0\n",
    "            insert = H[i,1:j].max() - gap_cost if j > 1 else 0\n",
    "            H[i,j] = max(match, delete, insert, 0)\n",
    "  # The highest score is the best local alignment.\n",
    "  # For our purposes, we don't actually care _what_ the alignment was, just how\n",
    "  # aligned the two strings were.\n",
    "    return H.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSWscore(query, long_text):\n",
    "    \"\"\"\n",
    "    param: query is the search query as a string.\n",
    "    param: text is the long text to compute the similarity.\n",
    "    return the number of significant alignment strings in both text. ie. the number of similar terms in query and long_text.\n",
    "    \"\"\"\n",
    "    query_ls = query.split()\n",
    "    long_text_ls = long_text.split()\n",
    "    \n",
    "    sw_score = []\n",
    "    for i in query_ls:\n",
    "        score = sum([smith_waterman(i,j)for j in long_text_ls if smith_waterman(i,j)>=4.0 ])\n",
    "        #print([smith_waterman(i,j) if smith_waterman(i,j)>=4.0 else 0 for j in long_text_ls])\n",
    "        sw_score.append(score)\n",
    "    return round(sum(sw_score)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSWscoreCol(df,query_col_name,long_text_col_name,new_col_name):\n",
    "    first_col = df[query_col_name]\n",
    "    second_col = df[long_text_col_name]\n",
    "    \n",
    "    score_ls = []\n",
    "    for i in range(len(first_col)):\n",
    "        score_ls.append(getSWscore(first_col.iloc[i],second_col.iloc[i]))\n",
    "    df[new_col_name] = score_ls\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SW score for search and title.\n",
    "newdf = createSWscoreCol(df_train_desc,\"search_term\",\"product_title\",\"search_title_SW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to write out the three new features.\n",
    "three_feature = newdf.drop([\"product_title\",\"search_term\",\"relevance\",\"product_description\"],axis=1)\n",
    "three_feature.to_csv(\"three_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SW score for search and description.\n",
    "newdf2 = createSWscoreCol(df_train_desc,\"search_term\",\"product_description\",\"search_desc_SW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to write out the four new features.\n",
    "newdf2.drop([\"product_title\",\"search_term\",\"relevance\",\"product_description\"],axis=1).to_csv(\"four_features.csv\")\n",
    "newdf2.to_csv(\"four_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Normalized compression distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized compression distance is a way to describe the similarity between any files by comparing the bit sizes. The NCD is a non-negative number 0 ≤ r ≤ 1 representing how different the two files are. Smaller numbers represent more similar files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import sys\n",
    "\n",
    "def computeNCD(string1,string2):\n",
    "    \"\"\"\n",
    "    params: string1 is the query term\n",
    "    params: string2 is the word in long-text, like title, description.\n",
    "    \"\"\"\n",
    "    # Get concated strings and transform to bytes-like object for lzma.compress.\n",
    "    concat_str = string1+string2\n",
    "    string1 = bytes(string1,'utf-8')\n",
    "    string2 = bytes(string2,'utf-8')\n",
    "    concat_str = bytes(concat_str,'utf-8')\n",
    "    \n",
    "    # Get the compressed file for each string.\n",
    "    str1_comp = lzma.compress(string1)  # compress file 1\n",
    "    str2_comp = lzma.compress(string2)  # compress file 2\n",
    "    concat_str_comp = lzma.compress(concat_str)  # compress file concatenated\n",
    "\n",
    "    # print len() of each file\n",
    "    # print(len(str1_comp), len(str2_comp), len(concat_str_comp), sep=' ', end='\\n')\n",
    "\n",
    "    # magic happens here\n",
    "    ncd = (len(concat_str_comp) - min(len(str1_comp), len(str2_comp))) / \\\n",
    "    max(len(str1_comp), len(str2_comp))\n",
    "\n",
    "    # print(ncd)\n",
    "    return ncd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test for computeNCD function\n",
    "str1 = df_train_desc.iloc[0][1] # the title\n",
    "str2 = df_train_desc.iloc[0][2] # the search\n",
    "str3 = df_train_desc.iloc[0][4] # the desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The usage of Normalized Compression Distance:</t>\n",
    "Since we only compare the bits difference from one string to another, so we need to average the score between word of query and word in title or description. So we need to find the overall similarity between the two words. To do so, I will compute each combination between word in query and word in long-text, and take the average as a final similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNCDCol(df,search_name, long_text_name,new_col_name):\n",
    "    NCD_score_ls = []\n",
    "    for i in range(len(df)):\n",
    "        str1 = df[search_name].iloc[i]\n",
    "        str2 = df[long_text_name].iloc[i]\n",
    "        NCD_score_ls.append(np.mean([computeNCD(a,b) for a in str1.split() for b in str2.split()]))\n",
    "    df[new_col_name] = NCD_score_ls\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try on first 100 lines of df_train_desc\n",
    "tryout = createNCDCol(df_train_desc,\"search_term\",\"product_title\",\"NCD_query_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
