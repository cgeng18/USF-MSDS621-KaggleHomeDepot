{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Check-in 2018-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Name: Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Names\n",
    "1. Jian Wang\n",
    "2. Chong Geng\n",
    "3. Alan Perry\n",
    "4. Divya Bhargavi\n",
    "5. Robert Sandor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import operator\n",
    "import re\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(file):\n",
    "    '''\n",
    "    Initiate the glove model as a dictionary\n",
    "    input: A String which is a file in the project directory\n",
    "    returns: A dictionary with item = word : 300 d list\n",
    "    '''\n",
    "    vecs = defaultdict(lambda: np.zeros(shape=(300,1)))\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for word_and_vec in lines:\n",
    "            elems = word_and_vec.strip().split(' ')\n",
    "            word = elems[0]\n",
    "            vec = np.array(elems[1:], dtype=float)\n",
    "            vecs[word] = vec\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dictionary():\n",
    "    \"\"\"\n",
    "    firstly, I split the dictionary into a wordlist and a matrix.\n",
    "    returns a list of words and \n",
    "    a 2d matrix of the normalized word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    wordlist = []\n",
    "    matrix = []\n",
    "    with open(glove_file) as f:\n",
    "        lines = f.readlines()\n",
    "        for word_and_vec in lines:\n",
    "            wordvec = np.array([float(x) for x in word_and_vec.split()[1:]])    \n",
    "            matrix.append(wordvec / np.linalg.norm(wordvec))\n",
    "            wordlist.append(word_and_vec.split()[0])\n",
    "        matrix = np.array(matrix)\n",
    "    return wordlist, matrix\n",
    "\n",
    "def unique_words(train_df):\n",
    "    \"\"\"\n",
    "    I then obtain the unique words that appear in the search_term.\n",
    "    \"\"\"\n",
    "    cleaned = list(train_df['cleaned_terms'])\n",
    "    all_words = []\n",
    "    for t in cleaned:\n",
    "        all_words += t.split(' ')\n",
    "\n",
    "    return list(set(all_words))[1:]\n",
    "\n",
    "def find_nearest_neighbors(filename, cleaned_set, matrix, wordlist, dictionary):\n",
    "    \"\"\"\n",
    "    here I count the cos_distance of each word that is in the cleaned_set.\n",
    "    the output file looks like (each line): w0, w1, w2, w3, w4,\n",
    "    i didn't print the distance, just the neighbour words\n",
    "    this will take couple of minutes.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"w\")\n",
    "    f.close()\n",
    "    for word in cleaned_set:\n",
    "        dots = matrix.dot(dictionary[word])\n",
    "        close_index_vec = np.argsort(dots)\n",
    "        for i in range(5):\n",
    "            f = open(filename, \"a\")\n",
    "            f.write(wordlist[int(close_index_vec[-1-i])])\n",
    "            f.write(',')\n",
    "            f.close()\n",
    "        f = open(filename, \"a\")\n",
    "        f.write('\\n')\n",
    "        f.close()  \n",
    "\n",
    "def get_all_terms_neighbors(dictionary, cleaned):\n",
    "    \"\"\"\n",
    "    terms_neighbour is the list which stores the top 4 neighbours of each searching_terms. \n",
    "    for example, if the searching term is: cleaned[0]='w1_w2', \n",
    "    then the terms_neighbour[0]='n11_n12_n13_n14_n21_n22_n23_n24'.\n",
    "    \"\"\"\n",
    "    terms_neighbour = []\n",
    "    for i in range(len(cleaned)):\n",
    "        neighbours = ''\n",
    "        if cleaned[i] != '':\n",
    "            words = cleaned[i].split(' ')\n",
    "            for w in words:\n",
    "                neighbours = neighbours + dictionary[w]+' '\n",
    "        terms_neighbour.append(neighbours)\n",
    "    return terms_neighbour\n",
    "\n",
    "def build_dictionary(file):\n",
    "    \"\"\"\n",
    "    based on the above output file, I then built a dictionary;\n",
    "    this dictionary stores each word (as key) with its top 4 neighbour words (as value) \n",
    "    \"\"\"\n",
    "    # use a defaultdict instead?\n",
    "    # k_dic = dict()\n",
    "    k_dic = defaultdict(lambda: '')\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = line.strip().split(',')\n",
    "            k_dic[words[0]] = words[1] + ' ' + words[2] + ' ' + words[3] + ' ' + words[4]\n",
    "    return k_dic\n",
    "\n",
    "def clean_term_in_doc(terms:list, title:list):\n",
    "    \"\"\"\n",
    "    This cleans the given terms  in the specified document\n",
    "    \"\"\"\n",
    "    count=np.zeros(len(terms))\n",
    "    for i in range(len(terms)):\n",
    "        if not pd.isnull(terms[i]): \n",
    "            title[i]=title[i].lower()\n",
    "            for term in terms[i].split(' '):\n",
    "                if term in title[i].split(' '):\n",
    "                    count[i]+=1\n",
    "    return count\n",
    "\n",
    "def get_length(column):\n",
    "    \"\"\"\n",
    "    This calculates and returns the number of words\n",
    "    for each row in a specified column\n",
    "    \"\"\"\n",
    "    length = np.zeros(len(column))\n",
    "    for index in range(len(column)):\n",
    "        if not pd.isnull(column[index]):\n",
    "            length[index] = len(column[index].split(' '))\n",
    "    return length\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    stops = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if (len(w) > 2 and (w not in stops))]  # ignore a, an, to, at, be, ...\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train_df, products_df, dictionary):\n",
    "    \"\"\"\n",
    "    Adds the following features to the training set dataframe: \n",
    "    clean_length (the number of words in the 'cleaned' search terms)\n",
    "    title_length (the number of words in the 'cleaned' title)\n",
    "    desc_length (the number of words in the 'cleaned' description)\n",
    "    clean_terms_in_title (the number of time \n",
    "    any of the words in clean_terms appears in the title)\n",
    "    clean_terms_in_desc (the number of time \n",
    "    any of the words in clean_terms appears in the description)\n",
    "    \"\"\"\n",
    "    # join the dataframes together\n",
    "    train_df = train_df.set_index('product_uid').join(products_df.set_index('product_uid'))\n",
    "    train_df = train_df.reset_index()\n",
    "    \n",
    "    # \"clean\" the search terms of numbers and stop words\n",
    "    search_terms = train_df['search_term']\n",
    "    cleaned_terms = [' '.join(tokenize(search_term)) for search_term in search_terms]\n",
    "    train_df['cleaned_terms'] = cleaned_terms\n",
    "    \n",
    "    cleaned = list(train_df['cleaned_terms'])\n",
    "    title = list(train_df['product_title'])\n",
    "    desc = list(train_df['product_description'])\n",
    "\n",
    "    # set up the calculations for finding the nearest neighbors\n",
    "    wordlist, matrix = split_dictionary()\n",
    "    cleaned_set = unique_words(train_df)\n",
    "    find_nearest_neighbors('glove_neighbour_no_w.txt', cleaned_set, matrix, wordlist, dictionary)\n",
    "    k_dict = build_dictionary('glove_neighbour_no_w.txt')\n",
    "    terms_neighbour = get_all_terms_neighbors(k_dict, cleaned)\n",
    "    train_df['terms_neighbour'] = terms_neighbour\n",
    "    \n",
    "    # create the features to be used in the model\n",
    "    train_df['clean_length'] = get_length(cleaned)\n",
    "    train_df['title_length'] = get_length(title)\n",
    "    train_df['desc_length'] = get_length(desc)\n",
    "    train_df['clean_terms_in_title'] = clean_term_in_doc(cleaned, title)\n",
    "    train_df['clean_terms_in_desc'] = clean_term_in_doc(cleaned, desc)\n",
    "    train_df['neighbours_in_title'] = clean_term_in_doc(terms_neighbour, title)\n",
    "    train_df['neighbours_in_desc'] = clean_term_in_doc(terms_neighbour, desc)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('product_descriptions.csv')\n",
    "train = pd.read_csv('train.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.300d.txt'\n",
    "glove_dic = make_dictionary(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_train = feature_engineering(train, products, glove_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = modified_train[['clean_length', 'title_length', \n",
    "                          'desc_length', 'clean_terms_in_title', \n",
    "                          'clean_terms_in_desc', 'neighbours_in_title',\n",
    "                         'neighbours_in_desc']]\n",
    "y_train = modified_train[['relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we can't see the relevancy scores of the test set,\n",
    "# I decided to split the training set \n",
    "train_data, test_data, train_target, test_target = train_test_split(X_train,\n",
    "                                                                        y_train,\n",
    "                                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "lin_reg_model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.08876602]\n",
      " [2.31638285]\n",
      " [2.48718285]\n",
      " [2.64340562]\n",
      " [2.82535515]]\n",
      "       relevance\n",
      "13534       3.00\n",
      "29748       2.67\n",
      "20225       2.67\n",
      "5169        2.67\n",
      "49860       2.00\n"
     ]
    }
   ],
   "source": [
    "predicted = lin_reg_model.predict(test_data)\n",
    "print(predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5101904093896746"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since an RMSE function couldn't be found quickly in the sklearn library,\n",
    "# we just used the MSE function and took the square root of that\n",
    "rmse_lin_reg = sqrt(mean_squared_error(predicted, test_target))\n",
    "\n",
    "# this value is equivalent to rank 1680 on the Kaggle leaderboard for this competition\n",
    "# the benchmark was ~ rank 1681\n",
    "# https://www.kaggle.com/c/home-depot-product-search-relevance/leaderboard\n",
    "rmse_lin_reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
