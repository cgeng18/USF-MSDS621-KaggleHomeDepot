{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Check-in 2018-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Name: Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Names\n",
    "1. Jian Wang\n",
    "2. Chong Geng\n",
    "3. Alan Perry\n",
    "4. Divya Bhargavi\n",
    "5. Robert Sandor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:13.740946Z",
     "start_time": "2018-12-02T07:40:11.780126Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import operator\n",
    "import re\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:13.754038Z",
     "start_time": "2018-12-02T07:40:13.743485Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_dictionary(file):\n",
    "    '''\n",
    "    Initiate the glove model as a dictionary\n",
    "    input: A String which is a file in the project directory\n",
    "    returns: A dictionary with item = word : 300 d list\n",
    "    \n",
    "    :param file:            the filepath string of the dictionary\n",
    "    :returns:               a dictionary with words as keys \n",
    "                            and 300d vectors as values\n",
    "    '''\n",
    "    vecs = defaultdict(lambda: np.zeros(shape=(300,1)))\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for word_and_vec in lines:\n",
    "            elems = word_and_vec.strip().split(' ')\n",
    "            word = elems[0]\n",
    "            vec = np.array(elems[1:], dtype=float)\n",
    "            vecs[word] = vec\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:14.290185Z",
     "start_time": "2018-12-02T07:40:14.269114Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dictionary():\n",
    "    \"\"\"\n",
    "    firstly, I split the dictionary into a wordlist and a matrix.\n",
    "    returns a list of words and \n",
    "    a 2d matrix of the normalized word vectors\n",
    "    \n",
    "    :returns:               the words and matrix associated with\n",
    "                            the glove dictionary\n",
    "    \"\"\"\n",
    "    wordlist = []\n",
    "    matrix = []\n",
    "    with open(glove_file) as f:\n",
    "        lines = f.readlines()\n",
    "        for word_and_vec in lines:\n",
    "            wordvec = np.array([float(x) for x in word_and_vec.split()[1:]])    \n",
    "            matrix.append(wordvec / np.linalg.norm(wordvec))\n",
    "            wordlist.append(word_and_vec.split()[0])\n",
    "        matrix = np.array(matrix)\n",
    "    return wordlist, matrix\n",
    "\n",
    "def unique_words(train_df):\n",
    "    \"\"\"\n",
    "    I then obtain the unique words that appear in the search_term.\n",
    "    \n",
    "    :param train_df:        the training set Pandas dataframe\n",
    "    :returns:               a list of unique words from search terms\n",
    "                            that have been stripped of numbers, symbols, etc.\n",
    "    \"\"\"\n",
    "    cleaned = list(train_df['cleaned_terms'])\n",
    "    all_words = []\n",
    "    for t in cleaned:\n",
    "        all_words += t.split(' ')\n",
    "\n",
    "    return list(set(all_words))[1:]\n",
    "\n",
    "def find_nearest_neighbors(filename, cleaned_set, matrix, wordlist, dictionary):\n",
    "    \"\"\"\n",
    "    here I count the cos_distance of each word that is in the cleaned_set.\n",
    "    the output file looks like (each line): w0, w1, w2, w3, w4,\n",
    "    i didn't print the distance, just the neighbour words\n",
    "    this will take couple of minutes.\n",
    "    \n",
    "    :param filename:        a string representing the filename to write to\n",
    "    :param cleaned_set:     a list of search terms that have \n",
    "                            been stripped of numbers, symbols, etc.\n",
    "    :param matrix:          a 2d Numpy array of the word vectors in wordlist\n",
    "    :param wordlist:        a list of words from the glove dictionary\n",
    "    :param dictionary:      a dictionary with words as keys \n",
    "                            and 300d vectors as values\n",
    "    \"\"\" \n",
    "    output_string = ''\n",
    "    \n",
    "    for word in cleaned_set:\n",
    "        dots = matrix.dot(dictionary[word])\n",
    "        close_index_vec = np.argsort(dots)\n",
    "        for i in range(5):\n",
    "            output_string += wordlist[int(close_index_vec[-1-i])] + ','\n",
    "        output_string += '\\n'\n",
    "        \n",
    "    f = open(filename, \"w\")\n",
    "    f.write(output_string)\n",
    "    f.close()\n",
    "\n",
    "def get_all_terms_neighbors(dictionary, cleaned):\n",
    "    \"\"\"\n",
    "    terms_neighbour is the list which stores the top 4 neighbours of each searching_terms. \n",
    "    for example, if the searching term is: cleaned[0]='w1_w2', \n",
    "    then the terms_neighbour[0]='n11_n12_n13_n14_n21_n22_n23_n24'.\n",
    "    \n",
    "    :param dictionary:      a dictionary\n",
    "    :param cleaned:         a list of search terms that have\n",
    "                            been stripped of numbers, symbols, etc.\n",
    "    :returns:               a list of concatenated words that are neighbors\n",
    "                            of the 'cleaned' terms\n",
    "    \"\"\"\n",
    "    terms_neighbour = []\n",
    "    for i in range(len(cleaned)):\n",
    "        neighbours = ''\n",
    "        if cleaned[i] != '':\n",
    "            words = cleaned[i].split(' ')\n",
    "            for w in words:\n",
    "                neighbours = neighbours + dictionary[w] + ' '\n",
    "        terms_neighbour.append(neighbours)\n",
    "    return terms_neighbour\n",
    "\n",
    "def build_dictionary(file):\n",
    "    \"\"\"\n",
    "    based on the above output file, I then built a dictionary;\n",
    "    this dictionary stores each word (as key) with its top 4 neighbour words (as value) \n",
    "    \n",
    "    :param file:            the file containing the list of strings of neighbors\n",
    "    :returns:               a dictionary with words as keys \n",
    "                            and 4 neighbors of that word as values\n",
    "    \"\"\"\n",
    "    k_dic = defaultdict(lambda: '')\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = line.strip().split(',')\n",
    "            k_dic[words[0]] = words[1] + ' ' + words[2] + ' ' + words[3] + ' ' + words[4]\n",
    "    return k_dic\n",
    "\n",
    "def clean_term_in_doc(terms, title):\n",
    "    \"\"\"\n",
    "    This cleans the given terms in the specified document\n",
    "    \n",
    "    :param terms:           a list of unique search terms\n",
    "    :param title:           a list of titles of products\n",
    "    :return:                a list of the counts of the \n",
    "                            cleaned terms within a product's title\n",
    "    \"\"\"\n",
    "    count = np.zeros(len(terms))\n",
    "    for i in range(len(terms)):\n",
    "        if not pd.isnull(terms[i]): \n",
    "            title[i] = title[i].lower()\n",
    "            for term in terms[i].split(' '):\n",
    "                if term in title[i].split(' '):\n",
    "                    count[i] += 1\n",
    "    return count\n",
    "\n",
    "def get_length(column):\n",
    "    \"\"\"\n",
    "    This calculates and returns the number of words\n",
    "    for each row in a specified column\n",
    "    \n",
    "    :param column:          the feature/attribute which\n",
    "                            will have its words counted\n",
    "    :returns:               a column with the count of \n",
    "                            words in each string\n",
    "    \"\"\"\n",
    "    length = np.zeros(len(column))\n",
    "    for index in range(len(column)):\n",
    "        if not pd.isnull(column[index]):\n",
    "            length[index] = len(column[index].split(' '))\n",
    "    return length\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \n",
    "    :param text:            a string\n",
    "    :returns:               the same string stripped of numbers,\n",
    "                            tabs, newline characters, and punctuation\n",
    "    \"\"\"\n",
    "    stops = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if (len(w) > 2 and (w not in stops))]  # ignore a, an, to, at, be, ...\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:18.114233Z",
     "start_time": "2018-12-02T07:40:18.104482Z"
    }
   },
   "outputs": [],
   "source": [
    "def letter_prob(phrases):\n",
    "    \"\"\"\n",
    "    :param phrases:         a list of strings of text\n",
    "    :returns:               a list of dictionaries of probabilities for characters in the text \n",
    "    \"\"\"\n",
    "    letter_counters = []\n",
    "    for phrase in phrases:\n",
    "        letter_count = defaultdict(lambda: 0)\n",
    "        for char in phrase:\n",
    "            if char.isalpha():\n",
    "                if char in letter_count:\n",
    "                    letter_count[char] += 1\n",
    "                else:\n",
    "                    letter_count[char] = 1\n",
    "        letter_counters.append(letter_count)\n",
    "        \n",
    "        total_count = float(sum(list(letter_count.values())))\n",
    "    \n",
    "        for key in letter_count.keys():\n",
    "            letter_count[key] = letter_count[key] / total_count\n",
    "            \n",
    "    return letter_counters\n",
    "\n",
    "def calculate_kl_divergences(P, Q):\n",
    "    \"\"\"\n",
    "    :param P:               a list of dictionaries of probabilities for a distribution\n",
    "    :param Q:               a list of dictionaries of probabilities for a second distribution\n",
    "    :returns:               a list of the kl-divergences between P and Q\n",
    "    \"\"\"\n",
    "    kl_divergences = []\n",
    "    for idx, q in enumerate(Q):\n",
    "        kl_divergence = 0\n",
    "        for key in q.keys():\n",
    "            if P[idx][key] != 0:\n",
    "                kl_divergence += P[idx][key] * math.log2(P[idx][key] / q[key])\n",
    "        kl_divergence *= -1\n",
    "        kl_divergences.append(kl_divergence)\n",
    "    return kl_divergences\n",
    "\n",
    "def calculate_entropy(probs_list):\n",
    "    \"\"\"\n",
    "    :param probs_list:      a list of dictionaries in which the values are probabilities\n",
    "    :returns:               a list of entropies calculated for the given probs_list\n",
    "    \"\"\"\n",
    "    entropies = []\n",
    "    for distribution in probs_list:\n",
    "        entropy = 0\n",
    "        for key in distribution.keys():\n",
    "            entropy += distribution[key] * math.log2(distribution[key])\n",
    "        entropy *= -1\n",
    "        entropies.append(entropy)\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:19.981714Z",
     "start_time": "2018-12-02T07:40:19.974187Z"
    }
   },
   "outputs": [],
   "source": [
    "def longest_common_subsequence(X , Y): \n",
    "    \"\"\"\n",
    "    :param X:               a list of strings of text\n",
    "    :param Y:               a list of strings of text\n",
    "    :returns:               a list of the integer length of the longest common subsequence \n",
    "                            between the strings\n",
    "    \"\"\"\n",
    "    lcs = []\n",
    "    \n",
    "    for idx, x in enumerate(X):\n",
    "        m = len(x) \n",
    "        n = len(Y[idx]) \n",
    " \n",
    "        L = [[None]*(n+1) for i in range(m+1)] \n",
    "\n",
    "        for i in range(m+1): \n",
    "            for j in range(n+1): \n",
    "                if i == 0 or j == 0 : \n",
    "                    L[i][j] = 0\n",
    "                elif x[i-1] == Y[idx][j-1]: \n",
    "                    L[i][j] = L[i-1][j-1]+1\n",
    "                else: \n",
    "                    L[i][j] = max(L[i-1][j] , L[i][j-1]) \n",
    "        lcs.append(L[m][n])\n",
    "                    \n",
    "    return lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:46:58.977641Z",
     "start_time": "2018-12-02T07:46:58.972781Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_index(text_1, text_2):\n",
    "    \"\"\"\n",
    "    :param text_1:         a list of strings of text\n",
    "    :param text_2:         a second list of strings of text\n",
    "    :returns:              a list of jaccard indices between \n",
    "                           the strings of text provided\n",
    "    \"\"\"\n",
    "    jaccard_indices = []\n",
    "    for text in zip(text_1, text_2):\n",
    "        tokens_1 = set(tokenize(text[0]))\n",
    "        tokens_2 = set(tokenize(text[1]))\n",
    "        intersection_ = tokens_1.intersection(tokens_2)\n",
    "        union_ = tokens_1.union(tokens_2)\n",
    "        jaccard_indices.append(len(list(intersection_)) / float(len(list(union_))))\n",
    "    return jaccard_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:26.829055Z",
     "start_time": "2018-12-02T07:40:26.818777Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(train_df, products_df, dictionary):\n",
    "    \"\"\"\n",
    "    Adds the following features to the training set dataframe: \n",
    "    * clean_length: the count of words in the 'cleaned' search terms\n",
    "    * title_length: the count of words in the 'cleaned' title\n",
    "    * desc_length: the count of words in the 'cleaned' description\n",
    "    * clean_terms_in_title: the number of time \n",
    "    any of the words in clean_terms appears in the title\n",
    "    * clean_terms_in_desc: the number of time \n",
    "    any of the words in clean_terms appears in the description\n",
    "    * neighbours_in_title: the count of the appearance of the \n",
    "    words closest to the search terms in the title\n",
    "    * neighbours_in_desc: the count of the appearance of the \n",
    "    words closest to the search terms in the description\n",
    "    \n",
    "    :param train_df:        the training set Pandas dataframe\n",
    "    :param products_df:     the product descriptions dataframe\n",
    "    :param dictionary:      the glove dictionary\n",
    "    :returns:               the modified dataframe with the additional features\n",
    "    \"\"\"\n",
    "    # join the dataframes together\n",
    "    train_df = train_df.set_index('product_uid').join(products_df.set_index('product_uid'))\n",
    "    train_df = train_df.reset_index()\n",
    "    \n",
    "    # \"clean\" the search terms of numbers and stop words\n",
    "    search_terms = train_df['search_term']\n",
    "    cleaned_terms = [' '.join(tokenize(search_term)) for search_term in search_terms]\n",
    "    train_df['cleaned_terms'] = cleaned_terms\n",
    "    \n",
    "    cleaned = list(train_df['cleaned_terms'])\n",
    "    title = list(train_df['product_title'])\n",
    "    desc = list(train_df['product_description'])\n",
    "    \n",
    "\n",
    "\n",
    "    # set up the calculations for finding the nearest neighbors\n",
    "    wordlist, matrix = split_dictionary()\n",
    "    cleaned_set = unique_words(train_df)\n",
    "    find_nearest_neighbors('glove_neighbour_no_w.txt', cleaned_set, matrix, wordlist, dictionary)\n",
    "    k_dict = build_dictionary('glove_neighbour_no_w.txt')\n",
    "    terms_neighbour = get_all_terms_neighbors(k_dict, cleaned)\n",
    "    train_df['terms_neighbour'] = terms_neighbour\n",
    "    \n",
    "    # create the features to be used in the model\n",
    "    train_df['clean_length'] = get_length(cleaned)\n",
    "    train_df['title_length'] = get_length(title)\n",
    "    train_df['desc_length'] = get_length(desc)\n",
    "    train_df['clean_terms_in_title'] = clean_term_in_doc(cleaned, title)\n",
    "    train_df['clean_terms_in_desc'] = clean_term_in_doc(cleaned, desc)\n",
    "    train_df['neighbours_in_title'] = clean_term_in_doc(terms_neighbour, title)\n",
    "    train_df['neighbours_in_desc'] = clean_term_in_doc(terms_neighbour, desc)\n",
    "    \n",
    "    # added extra features, including lcs, entropy and jaccard index\n",
    "    train_df['lcs_title'] = longest_common_subsequence(cleaned, title)\n",
    "    train_df['lcs_desc'] = longest_common_subsequence(cleaned, desc)\n",
    "    train_df['search_terms_entropy'] = calculate_entropy(letter_prob(cleaned))\n",
    "    train_df['title_entropy'] = calculate_entropy(letter_prob(title))\n",
    "    train_df['jaccard_index_title'] = calculate_jaccard_index(title, cleaned)\n",
    "    train_df['jaccard_index_desc'] = calculate_jaccard_index(desc, cleaned)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:29.308656Z",
     "start_time": "2018-12-02T07:40:28.095484Z"
    }
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('product_descriptions.csv')\n",
    "train = pd.read_csv('train.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T07:40:56.704853Z",
     "start_time": "2018-12-02T07:40:29.750220Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.300d.txt'\n",
    "glove_dic = make_dictionary(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T08:09:49.593932Z",
     "start_time": "2018-12-02T07:47:08.055611Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_train = feature_engineering(train, products, glove_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T08:09:49.670952Z",
     "start_time": "2018-12-02T08:09:49.601386Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = modified_train[['clean_length', 'title_length', \n",
    "                          'desc_length', 'clean_terms_in_title', \n",
    "                          'clean_terms_in_desc', 'neighbours_in_title',\n",
    "                         'neighbours_in_desc', 'title_entropy',\n",
    "                         'search_terms_entropy', 'lcs_title', 'lcs_desc',\n",
    "                         'jaccard_index_title', 'jaccard_index_desc']]\n",
    "y_train = modified_train[['relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T08:09:49.700998Z",
     "start_time": "2018-12-02T08:09:49.674811Z"
    }
   },
   "outputs": [],
   "source": [
    "# since we can't see the relevancy scores of the test set,\n",
    "# I decided to split the training set \n",
    "train_data, test_data, train_target, test_target = train_test_split(X_train,\n",
    "                                                                        y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T08:09:49.742059Z",
     "start_time": "2018-12-02T08:09:49.703867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "lin_reg_model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T08:09:49.766284Z",
     "start_time": "2018-12-02T08:09:49.745022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.63908569]\n",
      " [2.2400152 ]\n",
      " [2.47129219]\n",
      " [2.31350679]\n",
      " [2.50350051]]\n",
      "       relevance\n",
      "45055       3.00\n",
      "64720       1.00\n",
      "71594       3.00\n",
      "61619       2.33\n",
      "52166       2.67\n"
     ]
    }
   ],
   "source": [
    "predicted = lin_reg_model.predict(test_data)\n",
    "print(predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.876Z"
    }
   },
   "outputs": [],
   "source": [
    "# the following code was to test out AdaBoost and Bagging Trees\n",
    "# they eventually turned out to be less accurate than LinearRegression\n",
    "# but I provide them here to show what was done and the thought process behind it\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# models = [AdaBoostRegressor(),\n",
    "#          BaggingRegressor()]\n",
    "\n",
    "# grid_params = [{'n_estimators': range(1, 20, 3),\n",
    "#               'loss': ['linear', 'exponential'],\n",
    "#               'learning_rate': np.linspace(start=0.45, stop=.7, num=5)},\n",
    "#               {'n_estimators': range(10, 20, 3),\n",
    "#               'max_samples': np.linspace(start=0.2, stop=1.0, num=5),\n",
    "#               'max_features': np.linspace(start=0.25, stop=1.0, num=4)}]\n",
    "# best_models = []\n",
    "# for idx, model in enumerate(models):\n",
    "#     gs = GridSearchCV(estimator=model,\n",
    "#                      param_grid=grid_params[idx],\n",
    "#                      scoring='neg_mean_squared_error')\n",
    "#     gs.fit(train_data, train_target)\n",
    "#     best_models.append((gs.best_score_, gs.best_params_, models[idx]))\n",
    "# # {'learning_rate': 0.5499999999999999, 'loss': 'linear', 'n_estimators': 3}\n",
    "# # {'learning_rate': 0.6375, 'loss': 'exponential', 'n_estimators': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.879Z"
    }
   },
   "outputs": [],
   "source": [
    "# adaboost = AdaBoostRegressor(**best_models[0][1])\n",
    "# adaboost.fit(train_data, train_target)\n",
    "# adaboost_predicted = adaboost.predict(test_data)\n",
    "\n",
    "# bagging = BaggingRegressor(**best_models[1][1])\n",
    "# bagging.fit(train_data, train_target)\n",
    "# bagging_predicted = bagging.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.881Z"
    }
   },
   "outputs": [],
   "source": [
    "# after testing out Adaboost and learning that it performed\n",
    "# worse than LinearRegression, I decided to test out other\n",
    "# linear models; one of them (Lasso I believe) performed\n",
    "# equivalently and the others performed worse than LinearRegression\n",
    "\n",
    "# from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "# models_linear = [Lasso(),\n",
    "#                  Ridge(),\n",
    "#                  ElasticNet()]\n",
    "\n",
    "# grid_params_linear = [{'alpha': np.linspace(start=0.2, stop=1.0, num=5)},\n",
    "#                       {'alpha': np.linspace(start=0.2, stop=1.0, num=5),\n",
    "#                       'solver': ['svd', 'lsqr', 'sag', 'auto']},\n",
    "#                       {'alpha': np.linspace(start=0.2, stop=1.0, num=5),\n",
    "#                       'l1_ratio': np.linspace(start=0.2, stop=1.0, num=5),\n",
    "#                       'selection': ['cyclic', 'random']}]\n",
    "# best_models_linear = []\n",
    "# for idx, model in enumerate(models_linear):\n",
    "#     gs = GridSearchCV(estimator=model,\n",
    "#                      param_grid=grid_params_linear[idx],\n",
    "#                      scoring='neg_mean_squared_error')\n",
    "#     gs.fit(train_data, train_target)\n",
    "#     best_models_linear.append((gs.best_score_, gs.best_params_, models_linear[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.884Z"
    }
   },
   "outputs": [],
   "source": [
    "# lasso = Lasso(**best_models_linear[0][1])\n",
    "# lasso.fit(train_data, train_target)\n",
    "# lasso_predicted = lasso.predict(test_data)\n",
    "\n",
    "# ridge = Ridge(**best_models_linear[1][1])\n",
    "# ridge.fit(train_data, train_target)\n",
    "# ridge_predicted = ridge.predict(test_data)\n",
    "\n",
    "# elastic_net = ElasticNet(**best_models_linear[2][1])\n",
    "# elastic_net.fit(train_data, train_target)\n",
    "# elastic_net_predicted = elastic_net.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.886Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(best_models_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.888Z"
    }
   },
   "outputs": [],
   "source": [
    "# this was a rough test of RandomForest\n",
    "# and indicated that it performed better than Linear Regression\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100)\n",
    "# rf.fit(train_data, train_target)\n",
    "# rf_predicted = rf.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T08:09:49.774972Z",
     "start_time": "2018-12-02T08:09:49.768835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4971\n"
     ]
    }
   ],
   "source": [
    "rmse_lin_reg = sqrt(mean_squared_error(predicted, test_target))\n",
    "\n",
    "# the benchmark was ~ rank 1681 on the Kaggle Leaderboard\n",
    "# https://www.kaggle.com/c/home-depot-product-search-relevance/leaderboard\n",
    "# original set of features from checkin template: .5102\n",
    "# with only kl-divergence: .5130\n",
    "# with only title_entropy and search_entropy: .5086\n",
    "# with title_entropy, search_entropy, lcs_title, lcs_desc, jaccard_title, and jaccard_desc: .4971\n",
    "print(f\"{rmse_lin_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# rmse_adaboost = sqrt(mean_squared_error(adaboost_predicted, test_target))\n",
    "# print(f\"{rmse_adaboost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.893Z"
    }
   },
   "outputs": [],
   "source": [
    "# rmse_bagging = sqrt(mean_squared_error(bagging_predicted, test_target))\n",
    "# print(f\"{rmse_bagging:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.896Z"
    }
   },
   "outputs": [],
   "source": [
    "# rmse_lasso = sqrt(mean_squared_error(lasso_predicted, test_target))\n",
    "# print(f\"{rmse_lasso:.4f}\")\n",
    "\n",
    "# rmse_ridge = sqrt(mean_squared_error(ridge_predicted, test_target))\n",
    "# print(f\"{rmse_ridge:.4f}\")\n",
    "\n",
    "# rmse_elastic_net = sqrt(mean_squared_error(elastic_net_predicted, test_target))\n",
    "# print(f\"{rmse_elastic_net:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-01T23:29:29.898Z"
    }
   },
   "outputs": [],
   "source": [
    "# rmse_rf = sqrt(mean_squared_error(rf_predicted, test_target))\n",
    "# print(f\"{rmse_rf}:.4f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
