{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_with_distance_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = pd.read_csv('product_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "PARTIALS = False\n",
    "\n",
    "def gettext(xmltext):\n",
    "    \"\"\"\n",
    "    Parse xmltext and return the text from <title> and <text> tags\n",
    "    \"\"\"\n",
    "\n",
    "    xmltext = xmltext.encode('ascii', 'ignore') # ensure there are no weird char\n",
    "    root = ET.fromstring(xmltext)\n",
    "    text = []\n",
    "    for elem in root.iterfind('title'):\n",
    "        text.append(elem.text)\n",
    "    for elem in root.iterfind('.//text/*'):\n",
    "        text.append(elem.text)\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    stops = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if (len(w) > 2 and (w not in stops))]  # ignore a, an, to, at, be, ...\n",
    "    # print words\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def stemwords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list with each word\n",
    "    stemmed using a PorterStemmer.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(t) for t in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def tokenizer(text):\n",
    "    return stemwords(tokenize(text))\n",
    "\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    \"\"\"\n",
    "    Create and return a TfidfVectorizer object after training it on\n",
    "    the list of articles pulled from the corpus dictionary. The\n",
    "    corpus argument is a dictionary mapping file name to xml text.\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(input='content',\n",
    "                            analyzer='word',\n",
    "                            preprocessor=gettext,\n",
    "                            tokenizer=tokenizer,\n",
    "                            stop_words='english',\n",
    "                            decode_error='ignore')\n",
    "    tfidf.fit(list(corpus.values()))\n",
    "\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_column(train, product):\n",
    "    \"\"\"\n",
    "    Add the product description from product df to train df.\n",
    "    Concatenate Title and description to form total_description column.\n",
    "    \"\"\"\n",
    "    train = train.drop('Unnamed: 0', axis = 1)\n",
    "    train = train.set_index('product_uid').join(product.set_index('product_uid'))\n",
    "    train = train.reset_index()\n",
    "    product = product.reset_index()\n",
    "    train['total_description'] = train['product_title'] + train['product_description']\n",
    "    return train\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = description_column(train, product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter total description and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = train[['product_uid', 'total_description']]\n",
    "train_temp = train_temp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tfidf vectors for each product description and filter out 5 words with maximum tfidf scores for a product . This could help identify words that are unique to a product. CAUTION: The code below runs for 3-4 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function tokenizer at 0x110ede400>, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='content',\n",
    "                            analyzer='word',\n",
    "                            tokenizer=tokenizer,\n",
    "                            stop_words='english',\n",
    "                            decode_error='ignore')\n",
    "tfidf.fit(train_temp['total_description'])\n",
    "\n",
    "p = []\n",
    "total_description = list(train_temp['total_description'])\n",
    "for i in range(len(train_temp)):\n",
    "    response = tfidf.transform([total_description[i]])\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    col = response.nonzero()[1]\n",
    "    t = []\n",
    "    t = [(feature_names[col], response[0, col]) for col in response.nonzero()[1] if response[0, col] >= 0.09]\n",
    "    t.sort(key=lambda x: x[1], reverse=True)\n",
    "    p.append(t[0:5])\n",
    "    \n",
    "train_temp['tfidf'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(x):\n",
    "    \"\"\"\n",
    "    Remove the tfidf scores and return only the top tfidf words\n",
    "    \"\"\"\n",
    "    q = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i][0] != [] :\n",
    "            q.append(x[i][0])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are train_temp and train supposed to represent?\n",
    "# is train_temp just the product_uid and total_description columns?\n",
    "# why are the dataframes being joined?\n",
    "# is this just adding tfidf and num_words_in_description to the training set?\n",
    "def add_cols_to_train(train_temp, train):\n",
    "    train_temp['tfidf'] = train_temp['tfidf'].apply(lambda x: get_words(x))\n",
    "    \n",
    "    # is the line below duplication from cells above?\n",
    "    train_temp_1 = train_temp[['product_uid','tfidf']]\n",
    "    train_temp_1['tfidf'] = train_temp_1['tfidf'].apply( lambda x: ','.join(x))\n",
    "    train = train.set_index('product_uid').join(train_temp_1.set_index('product_uid'))\n",
    "    train = train.reset_index()\n",
    "    train_temp_1 = train_temp_1.reset_index()\n",
    "    train['tfidf'] = train['tfidf'].apply(lambda x: x.split(','))\n",
    "    train['num_words_in_description'] = train['total_description'].apply(lambda x: len(tokenize(x)))\n",
    "    \n",
    "    return train\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add tfidf column to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_cols_to_train(train_temp, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_stop_words(x):\n",
    "    from sklearn.feature_extraction import stop_words\n",
    "    stops = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "    return len([w for w in x if w in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_stop_words'] = train['search_term'].apply(lambda x: num_stop_words(x.split(' ')))\n",
    "train['num_search_words'] = train['search_term'].apply(lambda x: len(x.split(' ')))\n",
    "train['search_term_split'] = train['search_term'].apply(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf_words_in_search(train):\n",
    "    p = train['search_term_split']\n",
    "    q = train['tfidf']\n",
    "    l = []\n",
    "    for i in range(len(p)):\n",
    "        l.append(len(set(p[i]).intersection(set(q[i]))))\n",
    "    train['tfidf_search_common'] = l\n",
    "    \n",
    "    return train\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = find_tfidf_words_in_search(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_attrib_per_product(attributes):\n",
    "    \"\"\"\n",
    "    Find the number of attributes per product\n",
    "    \"\"\"\n",
    "    attributes['value'] = attributes['value'].apply(lambda x:tokenizer(str(x)))\n",
    "    attributes['value'] = attributes['value'].apply(lambda x: ','.join(x))\n",
    "    attrib_per_product = attributes.groupby('product_uid').agg(lambda x: x.tolist())\n",
    "    attrib_per_product = attrib_per_product.reset_index()\n",
    "    attrib_per_product['value'] = attrib_per_product['value'].apply(lambda x: ','.join(x).replace(',',' '))\n",
    "    attrib_per_product['num_attrib'] = attrib_per_product['name'].apply(lambda x: len(x))\n",
    "    attrib_per_product['value'].fillna('', inplace = True)\n",
    "    attrib_per_product.rename(columns = {'value':'attribs'})\n",
    "    attrib_per_product['product_uid'] = attrib_per_product['product_uid'].apply(lambda x: int(x))\n",
    "    \n",
    "    return attrib_per_product\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('attributes.csv', encoding='ISO-8859-1')\n",
    "attrib_per_product = num_attrib_per_product(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why are the dataframes being joined?\n",
    "train = train.set_index('product_uid').join(attrib_per_product.set_index('product_uid'))\n",
    "train = train.reset_index()\n",
    "attrib_per_product = attrib_per_product.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_with_tfidf.csv')\n",
    "combined = pd.read_csv('/Users/congchen/Downloads/combined_df-v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('Unnamed: 0', axis = 1)\n",
    "combined = combined.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = train[['product_uid','search_term','relevance', 'num_words_in_description','num_stop_words', 'num_search_words', 'tfidf_search_common','num_attrib']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74071"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_temp = train_temp.drop_duplicates()\n",
    "len(train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(combined, train_temp,  how='right', left_on=['product_uid','search_term','relevance'], right_on = ['product_uid','search_term','relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('final_combined.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
