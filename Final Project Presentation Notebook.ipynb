{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Presentation Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Name: Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Names\n",
    "1. Jian Wang\n",
    "2. Chong Geng\n",
    "3. Alan Perry\n",
    "4. Divya Bhargavi\n",
    "5. Robert Sandor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major tasks of this project was to generate numerical features based upon the text we were provided. After doing some research, we decided upon a number of features that we thought might work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:11:51.232080Z",
     "start_time": "2018-12-28T19:11:51.206031Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:11:55.882337Z",
     "start_time": "2018-12-28T19:11:53.797061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from Feature_Engineering.custom_estimators import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:11:59.289605Z",
     "start_time": "2018-12-28T19:11:56.126659Z"
    }
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('Data/product_descriptions.csv')\n",
    "train = pd.read_csv('Data/train.csv', encoding='ISO-8859-1')\n",
    "attributes = pd.read_csv('Data/attributes.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:12:49.841644Z",
     "start_time": "2018-12-28T19:11:59.292446Z"
    }
   },
   "outputs": [],
   "source": [
    "# BEWARE: this takes ~1 min to run\n",
    "attrib_per_product = attrib_stack(attributes, 'Data/attrib_per_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:00.521807Z",
     "start_time": "2018-12-28T19:12:49.844165Z"
    }
   },
   "outputs": [],
   "source": [
    "train, attrib_per_product = join_attrib(train, attrib_per_product)\n",
    "train = search_term_in_attrib(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:11.100032Z",
     "start_time": "2018-12-28T19:13:00.523898Z"
    }
   },
   "outputs": [],
   "source": [
    "train = color_df(attributes, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:42.493633Z",
     "start_time": "2018-12-28T19:13:11.108505Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_file = 'Data/glove.6B.300d.txt'\n",
    "glove_dic = make_dictionary(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:42.793298Z",
     "start_time": "2018-12-28T19:13:42.496750Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_train = train.set_index('product_uid').join(\n",
    "        products.set_index('product_uid'))\n",
    "modified_train = modified_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:42.920395Z",
     "start_time": "2018-12-28T19:13:42.795936Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_train['total_description'] = modified_train['product_title'] + \\\n",
    "        modified_train['product_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:43.039804Z",
     "start_time": "2018-12-28T19:13:42.922517Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('cleaned_pipeline', Pipeline([\n",
    "            ('cleaned_terms', CleanedTerms()),\n",
    "            ('secondary_cleaned_features', FeatureUnion([\n",
    "                ('cleaned_search_term_length', Length(\n",
    "                    'cleaned_search_term_length', 'cleaned_terms')),\n",
    "                ('title_entropy', Entropy('product_title')),\n",
    "                ('search_terms_entropy', Entropy('cleaned_terms')),\n",
    "                ('jscore_title', Jaro('search_term', 'product_title')),\n",
    "                ('jscore_desc', Jaro('search_term', 'product_description')),\n",
    "                ('clean_terms_in_title', FindTermsInCorpus(\n",
    "                    'cleaned_terms', 'product_title')),\n",
    "                ('clean_terms_in_desc', FindTermsInCorpus(\n",
    "                    'cleaned_terms', 'product_description')),\n",
    "                ('jaccard_index_title', JaccardIndex('product_title',\n",
    "                                                     'cleaned_terms')),\n",
    "                ('jaccard_index_desc', JaccardIndex('product_description',\n",
    "                                                    'cleaned_terms')),\n",
    "                ('lcs_title', LCS('cleaned_terms',\n",
    "                                  'product_title')),\n",
    "                ('lcs_desc', LCS('cleaned_terms',\n",
    "                                 'product_description'))\n",
    "            ])),\n",
    "        ])),\n",
    "        ('neighbours_pipeline', Pipeline([\n",
    "            ('cleaned_terms', CleanedTerms()),\n",
    "            ('terms_neighbour', FindNeighbors(\n",
    "                'cleaned_terms', glove_dic, glove_file)),\n",
    "            ('neighbors_numerical_features', FeatureUnion([\n",
    "                ('neighbors_in_title', FindNeighborsInCorpus('terms_neighbour',\n",
    "                                                             'product_title')),\n",
    "                ('neighbors_in_desc', FindNeighborsInCorpus('terms_neighbour',\n",
    "                                                            'product_description'))\n",
    "            ]))\n",
    "        ])),\n",
    "        ('stemmed_pipeline', Pipeline([\n",
    "            ('stemmed_terms', StemmedTerms('stemmed_terms', 'search_term')),\n",
    "            ('stemmed_title', StemmedTerms('stemmed_title', 'product_title')),\n",
    "            ('stemmed_desc', StemmedTerms('stemmed_desc', 'product_description')),\n",
    "            ('secondary_stemmed_features', FeatureUnion([\n",
    "                ('stemmed_terms_in_title', FindTermsInCorpus(\n",
    "                    'stemmed_terms', 'stemmed_title')),\n",
    "                ('stemmed_terms_in_desc', FindTermsInCorpus(\n",
    "                    'stemmed_terms', 'stemmed_desc'))\n",
    "            ]))\n",
    "        ])),\n",
    "        ('lemmatized_pipeline', Pipeline([\n",
    "            ('lemmatized_terms', LemmatizedTerms(\n",
    "                'lemmatized_terms', 'search_term')),\n",
    "            ('lemmatized_title', LemmatizedTerms(\n",
    "                'lemmatized_title', 'product_title')),\n",
    "            ('lemmatized_desc', LemmatizedTerms(\n",
    "                'lemmatized_desc', 'product_description')),\n",
    "            ('lemmatized_numerical_features', FeatureUnion([\n",
    "                ('lemmatized_terms_in_title', FindTermsInCorpus('lemmatized_terms',\n",
    "                                                                'lemmatized_title')),\n",
    "                ('lemmatized_terms_in_desc', FindTermsInCorpus('lemmatized_terms',\n",
    "                                                               'lemmatized_desc'))\n",
    "            ]))\n",
    "        ])),\n",
    "        ('miscellaneous_pipeline', Pipeline([\n",
    "            ('primary_misc_features', FeatureUnion([\n",
    "                ('num_words_in_description', CountWords(\n",
    "                    'total_description', lambda x: len(tokenize(x)))),\n",
    "                ('num_stop_words', CountWords('search_term',\n",
    "                                              lambda x: num_stop_words(x.split(' ')))),\n",
    "                ('num_search_words', CountWords(\n",
    "                    'search_term', lambda x: len(x.split(' ')))),\n",
    "                ('tfidf_search_common', TFIDFSearchIntersection()),\n",
    "                ('num_attributes', CountAttributes(attributes)),\n",
    "                ('title_length', Length('product_title_length', 'product_title')),\n",
    "                ('desc_length', Length('product_desc_length', 'product_description')),\n",
    "                ('min_levenstein_dist_title', MinLevensteinDistTitle()),\n",
    "                ('min_levenstein_dist_brand', MinLevensteinDistBrand(attributes)),\n",
    "                ('color_in_search_term', FindColorInSearchTerm(attributes)),\n",
    "                ('search_title_SW', SW_Score(\n",
    "                    'search_term', 'product_title')),  \n",
    "                ('search_desc_SW', SW_Score(\n",
    "                    'search_term', 'product_description')),\n",
    "                ('NCD_query_title', NCD('product_title', 'search_term'))\n",
    "            ]))\n",
    "        ]))\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:43.277145Z",
     "start_time": "2018-12-28T19:13:43.041824Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:43.332087Z",
     "start_time": "2018-12-28T19:13:43.279890Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = modified_train[['product_title', 'search_term',\n",
    "                          'name', 'value', 'search_term_split', 'search_term_in_attrib',\n",
    "                          'product_description', 'product_uid', 'total_description']]\n",
    "y_train = modified_train[['relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T19:13:43.391759Z",
     "start_time": "2018-12-28T19:13:43.334324Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(X_train,\n",
    "                                                                    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:14:14.452077Z",
     "start_time": "2018-12-28T19:13:43.393428Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  X['cleaned_terms'] = cleaned_terms\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  X['cleaned_terms'] = cleaned_terms\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  k_dict, list(X[self.terms]))\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  for word in X[self.orig_col_name]]\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  ' '.join(lemmatized(tokenize(word))) for word in X[self.orig_col_name]]\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/feature_engineering.py:860: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  train_set['tfidf'] = p\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  lambda x: tokenizer(x))\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  lambda x: list(set(tokenize(x))))\n",
      "/Users/robertsandor/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5434: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  lambda x: x.split(' '))\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  X['brand'] = X['product_uid'].apply(lambda x: d[x])\n",
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/custom_estimators.py:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  lambda x: x.split(' '))\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: takes ~4 hr to run (with all features)\n",
    "features = test_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becaues of the time-consuming nature of the feature engineering, we saved the data into a csv. If you don't want to run all of the previous functions (which takes ~4 hours to run), just start with the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:44.728792Z",
     "start_time": "2018-12-28T23:32:38.949573Z"
    }
   },
   "outputs": [],
   "source": [
    "zip_data = zipfile.ZipFile('Data/final_dataset.csv.zip')\n",
    "zip_data.extractall('Data/')\n",
    "modified_train = pd.read_csv('Data/final_dataset.csv')\n",
    "modified_train = modified_train.drop('Unnamed: 0', axis=1)\n",
    "modified_train.fillna(0, inplace=True)\n",
    "assert(len(modified_train.columns) == 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:44.811410Z",
     "start_time": "2018-12-28T23:32:44.730586Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = modified_train[['clean_length', 'title_length',\n",
    "                          'desc_length', 'clean_terms_in_title',\n",
    "                          'clean_terms_in_desc', \n",
    "                          'min_levenstein_dist_title', 'min_levenstein_dist_brand',\n",
    "                          'stemmed_terms_in_title', 'stemmed_terms_in_desc',\n",
    "                          'lemmatized_terms_in_title', 'lemmatized_terms_in_desc',\n",
    "                          'neighbours_in_title', 'neighbours_in_desc', 'search_terms_entropy',\n",
    "                          'title_entropy', 'jaccard_index_title', 'jaccard_index_desc', 'lcs_title',\n",
    "                          'lcs_desc', 'jscore_query_desc', 'jscore_query_title', 'search_title_SW',\n",
    "                          'search_desc_SW', 'NCD_query_title', 'num_words_in_description', 'num_stop_words',\n",
    "                          'num_search_words', 'tfidf_search_common', 'num_attrib']]\n",
    "y_train = modified_train[['relevance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can't see the relevancy scores of the test set, we decided to split the training set further into our own training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:45.335059Z",
     "start_time": "2018-12-28T23:32:44.814061Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(X_train,\n",
    "                                                                    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:45.520518Z",
     "start_time": "2018-12-28T23:32:45.471916Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(int(math.floor(len(X_train) * .75)) == len(train_data))\n",
    "assert(len(train_data) == len(train_target))\n",
    "assert(int(math.ceil(len(X_train) * .25)) == len(test_data))\n",
    "assert(len(test_data) == len(test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple baseline, we considered the RMSE of a completely random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:48.520049Z",
     "start_time": "2018-12-28T23:32:48.480322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74071\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train)) # no empty relevancy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform a randomization test on the relevancy scores where we permute the relevancy scores. In that way, we can keep the same distribution and effectively get a completely random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:51.244588Z",
     "start_time": "2018-12-28T23:32:51.199129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7544671945558852\n"
     ]
    }
   ],
   "source": [
    "random_score = np.random.permutation(y_train)\n",
    "random_chance_performance = math.sqrt(mean_squared_error(random_score, y_train))\n",
    "print(random_chance_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simplest model, we decided to use linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:58.952999Z",
     "start_time": "2018-12-28T23:32:58.794631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "lin_reg_model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:32:59.878188Z",
     "start_time": "2018-12-28T23:32:59.823472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.31324209]\n",
      " [2.4665354 ]\n",
      " [2.87651038]\n",
      " [2.77161773]\n",
      " [2.2364893 ]]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "predicted = lin_reg_model.predict(test_data)\n",
    "print(predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some exploration, we realized that some of the predictions produced by linear regression exceeded the bounds of the relevancy score metric. To account for that, we performed a min-max scaling to get the predictions within the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:33:05.672566Z",
     "start_time": "2018-12-28T23:33:05.629745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.20801308]\n",
      " [2.34930012]\n",
      " [2.72716496]\n",
      " [2.63048773]\n",
      " [2.13727174]]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(1.0, 3.0))\n",
    "scaled_linear_predicted = scaler.fit_transform(predicted)\n",
    "print(scaled_linear_predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, we considered alternative linear models that have regularization like Lasso, Ridge and ElasticNet to evaluate if there was overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:41:03.898208Z",
     "start_time": "2018-12-28T23:33:18.263166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5270565244813905, {'ls_reg__alpha': 0.25, 'ls_reg__normalize': False, 'ls_reg__selection': 'random'}, Pipeline(memory=None,\n",
      "     steps=[('ls_reg', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False))])), (0.4909474416460913, {'ridge_reg__alpha': 0.5, 'ridge_reg__normalize': False, 'ridge_reg__solver': 'svd'}, Pipeline(memory=None,\n",
      "     steps=[('ridge_reg', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001))])), (0.5143959951062845, {'en_reg__alpha': 0.5, 'en_reg__l1_ratio': 0.25, 'en_reg__normalize': False, 'en_reg__selection': 'cyclic'}, Pipeline(memory=None,\n",
      "     steps=[('en_reg', ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: this takes ~11m to run\n",
    "alt_linear_models = [Pipeline([('ls_reg', Lasso())]),\n",
    "                     Pipeline([('ridge_reg', Ridge())]),\n",
    "                     Pipeline([('en_reg' ,ElasticNet())])]\n",
    "\n",
    "alt_linear_grid_params = [{'ls_reg__alpha': np.linspace(start=0.25, stop=1.0, num=4),\n",
    "                           'ls_reg__normalize': [False, True],\n",
    "                           'ls_reg__selection': ['cyclic', 'random']},\n",
    "                          {'ridge_reg__alpha': np.linspace(start=0.5, stop=2.0, num=4),\n",
    "                           'ridge_reg__normalize': [False, True],\n",
    "                           'ridge_reg__solver': ['svd', 'lsqr', 'sag', 'saga']},\n",
    "                          {'en_reg__alpha': np.linspace(start=0.5, stop=2.0, num=4),\n",
    "                           'en_reg__l1_ratio': np.linspace(start=0.25, stop=1.0, num=4),\n",
    "                           'en_reg__normalize': [False, True],\n",
    "                           'en_reg__selection': ['cyclic', 'random']}]\n",
    "\n",
    "best_models_alt_linear = grid_search_models_rmse(alt_linear_models, alt_linear_grid_params, train_data, train_target)\n",
    "\n",
    "print(best_models_alt_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:41:03.955685Z",
     "start_time": "2018-12-28T23:41:03.900700Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_alt_linear = change_best_params_keys(best_models_alt_linear, \n",
    "                                                ['ridge_reg__alpha', 'ridge_reg__normalize', 'ridge_reg__solver'],\n",
    "                                                ['alpha', 'normalize', 'solver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:41:04.059880Z",
     "start_time": "2018-12-28T23:41:03.957746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.31352757]\n",
      " [2.46523882]\n",
      " [2.87508328]\n",
      " [2.77329945]\n",
      " [2.23622066]]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "alt_linear_predicted = fit_best_model(best_model_alt_linear, train_data, train_target, test_data)\n",
    "print(alt_linear_predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After considering various linear models, we decided to test if tree-based models could improve upon the performance of the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:58:45.253934Z",
     "start_time": "2018-12-28T23:41:04.062294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.4754440798246978, {'rf_reg__max_features': 'sqrt', 'rf_reg__n_estimators': 26}, Pipeline(memory=None,\n",
      "     steps=[('rf_reg', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))])), (0.4997047751670171, {'ab_reg__learning_rate': 0.5, 'ab_reg__loss': 'linear', 'ab_reg__n_estimators': 11}, Pipeline(memory=None,\n",
      "     steps=[('ab_reg', AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=None))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: this takes ~26m to run\n",
    "tree_models = [Pipeline([('rf_reg', RandomForestRegressor())]),\n",
    "               Pipeline([('ab_reg', AdaBoostRegressor())])]\n",
    "\n",
    "grid_params_tree = [{'rf_reg__n_estimators': range(1, 30, 5),\n",
    "                     'rf_reg__max_features': ['auto', 'sqrt', 'log2', None]},\n",
    "                    {'ab_reg__n_estimators': range(1, 30, 5),\n",
    "                     'ab_reg__loss': ['linear', 'square'],\n",
    "                     'ab_reg__learning_rate': np.linspace(start=0.5, stop=1.5, num=5)}]\n",
    "\n",
    "best_models_tree = grid_search_models_rmse(tree_models, grid_params_tree, train_data, train_target)\n",
    "\n",
    "print(best_models_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:58:45.379235Z",
     "start_time": "2018-12-28T23:58:45.256534Z"
    }
   },
   "outputs": [],
   "source": [
    "best_tree_model = change_best_params_keys(best_models_tree, \n",
    "                                          ['rf_reg__n_estimators', 'rf_reg__max_features'],\n",
    "                                          ['n_estimators', 'max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T23:58:49.418065Z",
     "start_time": "2018-12-28T23:58:45.382566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/feature_engineering.py:1044: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  best_model.fit(train_data, train_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.25653846 2.31384615 2.91038462 2.55269231 2.39692308]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "# here we chose the best tree model\n",
    "tree_predicted = fit_best_model(best_tree_model, train_data, train_target, test_data)\n",
    "print(tree_predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:09:17.120191Z",
     "start_time": "2018-12-28T23:58:49.420790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.46847941511634583, {'rf_reg__max_features': 'log2', 'rf_reg__n_estimators': 125}, Pipeline(memory=None,\n",
      "     steps=[('rf_reg', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE : this takes ~17m to run\n",
    "rf_models = [Pipeline([('rf_reg', RandomForestRegressor())])]\n",
    "\n",
    "grid_params_rf = [{'rf_reg__n_estimators': range(25, 151, 10),\n",
    "                   'rf_reg__max_features': ['log2']}]\n",
    "\n",
    "best_models_rf = grid_search_models_rmse(rf_models, grid_params_rf, train_data, train_target)\n",
    "\n",
    "print(best_models_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:09:17.252595Z",
     "start_time": "2018-12-29T00:09:17.122111Z"
    }
   },
   "outputs": [],
   "source": [
    "best_rf_model = change_best_params_keys(best_models_rf, \n",
    "                                        ['rf_reg__n_estimators', 'rf_reg__max_features'],\n",
    "                                        ['n_estimators', 'max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:09:32.986475Z",
     "start_time": "2018-12-29T00:09:17.255704Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/feature_engineering.py:1044: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  best_model.fit(train_data, train_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.24134667 2.35872    2.8776     2.57264    2.40992   ]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "rf_predicted = fit_best_model(best_rf_model, train_data, train_target, test_data)\n",
    "print(rf_predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be thorough, we also considered a model that used boosting (Gradient Boosting) to see how boosting would compare to all of our models thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:12:52.820213Z",
     "start_time": "2018-12-29T00:09:32.989423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.48007125970513725, {'gb_reg__learning_rate': 0.5, 'gb_reg__loss': 'ls', 'gb_reg__n_estimators': 21}, Pipeline(memory=None,\n",
      "     steps=[('gb_reg', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: this takes ~14m to run\n",
    "gb_model_pipeline = [Pipeline([('gb_reg', GradientBoostingRegressor())])]\n",
    "\n",
    "grid_params_gb = [{'gb_reg__loss': ['ls', 'huber'],\n",
    "                   'gb_reg__n_estimators': range(1, 26, 5),\n",
    "                   'gb_reg__learning_rate': [0.1, 0.25, 0.5, 0.75, 0.9]}]\n",
    "\n",
    "best_gb_models = grid_search_models_rmse(\n",
    "    gb_model_pipeline, grid_params_gb, train_data, train_target)\n",
    "\n",
    "print(best_gb_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:12:52.862114Z",
     "start_time": "2018-12-29T00:12:52.822005Z"
    }
   },
   "outputs": [],
   "source": [
    "best_gb_model = change_best_params_keys(best_gb_models, \n",
    "                                          ['gb_reg__learning_rate', 'gb_reg__n_estimators', 'gb_reg__loss'],\n",
    "                                          ['learning_rate', 'n_estimators', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:12:54.424015Z",
     "start_time": "2018-12-29T00:12:52.863791Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.43141544 2.37561736 2.69874211 2.64194775 2.47703813]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "gb_predicted = fit_best_model(best_gb_model, train_data, train_target, test_data)\n",
    "print(gb_predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:19:55.233136Z",
     "start_time": "2018-12-29T00:12:54.425909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.4759104270479097, {'gb_reg__learning_rate': 0.4, 'gb_reg__loss': 'ls', 'gb_reg__n_estimators': 150}, Pipeline(memory=None,\n",
      "     steps=[('gb_reg', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: this takes ~11m to run\n",
    "refined_gb_model_pipeline = [\n",
    "    Pipeline([('gb_reg', GradientBoostingRegressor())])]\n",
    "\n",
    "grid_params_refined_gb = [{'gb_reg__loss': ['ls'],\n",
    "                           'gb_reg__n_estimators': range(25, 151, 25),\n",
    "                           'gb_reg__learning_rate': [0.4, 0.5, 0.6]}]\n",
    "\n",
    "best_refined_gb_models = grid_search_models_rmse(\n",
    "    refined_gb_model_pipeline, grid_params_refined_gb, train_data, train_target)\n",
    "\n",
    "print(best_refined_gb_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We even considered XGBoost to see if that would get us the best RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:19:59.275043Z",
     "start_time": "2018-12-29T00:19:55.235154Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', booster='gblinear', reg_lambda=0.01)\n",
    "xg_reg.fit(train_data, train_target)\n",
    "xgb_predictions = xg_reg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:19:59.315717Z",
     "start_time": "2018-12-29T00:19:59.276919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5158811009929469\n"
     ]
    }
   ],
   "source": [
    "xgb_rmse = sqrt(mean_squared_error(xgb_predictions, test_target))\n",
    "print(xgb_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:20:10.959335Z",
     "start_time": "2018-12-29T00:19:59.317952Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', learning_rate=0.1, booster='gbtree', gamma=0.5, max_depth=5)\n",
    "xg_reg.fit(train_data, train_target)\n",
    "xgb_predictions = xg_reg.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (aka Neural Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we tested out a Multi-Layer Perceptron, or `sklearn`'s version of neural networks, for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:36:20.445921Z",
     "start_time": "2018-12-29T00:30:22.581203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.49174152911273955, {'nn_reg__activation': 'logistic', 'nn_reg__solver': 'adam'}, Pipeline(memory=None,\n",
      "     steps=[('nn_reg', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: this takes ~7m to run\n",
    "nn_model_pipeline = [Pipeline([('nn_reg', MLPRegressor())])]\n",
    "\n",
    "grid_params_nn = [{'nn_reg__activation': ['logistic', 'tanh', 'relu'],\n",
    "                   'nn_reg__solver': ['lbfgs', 'sgd', 'adam']}]\n",
    "\n",
    "best_nn_models = grid_search_models_rmse(nn_model_pipeline, grid_params_nn, train_data, train_target)\n",
    "\n",
    "print(best_nn_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a cursory look at the RMSE for the training portion, it didn't perform even close to what the tree-based models did and we decided not to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Models - Reduced Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating performance on Random Forest models, we wondered how applying PCA (Principal Component Analysis) to reduce the dimensionality of our data would affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:41:11.152712Z",
     "start_time": "2018-12-29T00:36:20.448231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.512190454365941, {'rf_reg__max_features': 'log2', 'rf_reg__n_estimators': 138}, Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('rf_reg', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_...timators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE: this takes ~6m to run\n",
    "transformed_rf_models = [Pipeline([('scale', StandardScaler()),\n",
    "                                   ('pca', PCA(n_components=int(\n",
    "                                       math.log2(len(X_train.columns))))),\n",
    "                                   ('rf_reg', RandomForestRegressor())])]\n",
    "\n",
    "grid_params_transformed_rf = [{'rf_reg__n_estimators': range(138, 143, 2),\n",
    "                               'rf_reg__max_features': ['log2']}]\n",
    "\n",
    "best_models_transformed_rf = grid_search_models_rmse(\n",
    "    transformed_rf_models, grid_params_transformed_rf, train_data, train_target)\n",
    "\n",
    "print(best_models_transformed_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:41:11.270529Z",
     "start_time": "2018-12-29T00:41:11.154524Z"
    }
   },
   "outputs": [],
   "source": [
    "best_transformed_rf_model = change_best_params_keys(best_models_transformed_rf, \n",
    "                                          ['rf_reg__n_estimators', 'rf_reg__max_features'],\n",
    "                                          ['n_estimators', 'max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T00:41:27.399452Z",
     "start_time": "2018-12-29T00:41:11.273299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/Documents/MSDS_Classes/MSDS621_Machine_Learning/KaggleHomeDepot/Feature_Engineering/feature_engineering.py:1044: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  best_model.fit(train_data, train_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.17403986 2.37243659 2.82630435 2.55514493 2.37086957]\n",
      "       relevance\n",
      "107         2.67\n",
      "52975       2.67\n",
      "17575       3.00\n",
      "71718       2.67\n",
      "23030       2.33\n"
     ]
    }
   ],
   "source": [
    "transformed_rf_predicted = fit_best_model(best_transformed_rf_model, train_data, train_target, test_data)\n",
    "print(transformed_rf_predicted[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Smaller Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to also test out how using only certain 'categories' of our features would affect our model. We grouped our model into similarity features (Jaro index, Jaccard index, etc.),  count features, and length/entropy features. \n",
    "\n",
    "We then tested out the best models using only those subsets of features to compare to the performance of the model with the full set of features. This is different than simply using PCA or other dimensionality reduction techniques because PCA may choose a few select features from each of the categories whereas this tests only features from one particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:20:34.614384Z",
     "start_time": "2018-12-07T01:20:34.584696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_terms</th>\n",
       "      <th>clean_terms_in_title</th>\n",
       "      <th>clean_terms_in_desc</th>\n",
       "      <th>stemmed_terms_in_title</th>\n",
       "      <th>stemmed_terms_in_desc</th>\n",
       "      <th>lemmatized_terms_in_title</th>\n",
       "      <th>lemmatized_terms_in_desc</th>\n",
       "      <th>neighbours_in_title</th>\n",
       "      <th>neighbours_in_desc</th>\n",
       "      <th>search_terms_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>jscore_query_desc</th>\n",
       "      <th>jscore_query_title</th>\n",
       "      <th>search_title_SW</th>\n",
       "      <th>search_desc_SW</th>\n",
       "      <th>NCD_query_title</th>\n",
       "      <th>num_words_in_description</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_search_words</th>\n",
       "      <th>tfidf_search_common</th>\n",
       "      <th>num_attrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angl bracket</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.251629</td>\n",
       "      <td>...</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.107077</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bracket</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107077</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deck</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.711111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  stemmed_terms  clean_terms_in_title  clean_terms_in_desc  \\\n",
       "0  angl bracket                   1.0                  0.0   \n",
       "1       bracket                   0.0                  0.0   \n",
       "2          deck                   0.0                  0.0   \n",
       "\n",
       "   stemmed_terms_in_title  stemmed_terms_in_desc  lemmatized_terms_in_title  \\\n",
       "0                     1.0                    1.0                        1.0   \n",
       "1                     0.0                    0.0                        0.0   \n",
       "2                     0.0                    1.0                        0.0   \n",
       "\n",
       "   lemmatized_terms_in_desc  neighbours_in_title  neighbours_in_desc  \\\n",
       "0                       1.0                  0.0                 1.0   \n",
       "1                       0.0                  0.0                 0.0   \n",
       "2                       1.0                  0.0                 1.0   \n",
       "\n",
       "   search_terms_entropy     ...      jscore_query_desc  jscore_query_title  \\\n",
       "0              3.251629     ...               2.833333            0.866667   \n",
       "1              2.807355     ...               0.000000            0.000000   \n",
       "2              2.000000     ...               2.711111            0.000000   \n",
       "\n",
       "   search_title_SW  search_desc_SW  NCD_query_title  num_words_in_description  \\\n",
       "0              1.0             4.0         0.107077                        79   \n",
       "1              0.0             0.0         0.107077                        79   \n",
       "2              0.0             3.0         0.109091                       109   \n",
       "\n",
       "   num_stop_words  num_search_words  tfidf_search_common  num_attrib  \n",
       "0               0                 2                    1        15.0  \n",
       "1               0                 2                    0        15.0  \n",
       "2               1                 2                    1        35.0  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_num_features = modified_train[getAllNumericalCols(modified_train)]\n",
    "all_num_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:27:13.476326Z",
     "start_time": "2018-12-07T01:27:13.459979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbours_in_title</th>\n",
       "      <th>neighbours_in_desc</th>\n",
       "      <th>jaccard_index_title</th>\n",
       "      <th>jaccard_index_desc</th>\n",
       "      <th>lcs_title</th>\n",
       "      <th>lcs_desc</th>\n",
       "      <th>jscore_query_desc</th>\n",
       "      <th>jscore_query_title</th>\n",
       "      <th>search_title_SW</th>\n",
       "      <th>search_desc_SW</th>\n",
       "      <th>NCD_query_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.107077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.711111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.109091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neighbours_in_title  neighbours_in_desc  jaccard_index_title  \\\n",
       "0                  0.0                 1.0             0.166667   \n",
       "1                  0.0                 0.0             0.000000   \n",
       "2                  0.0                 1.0             0.000000   \n",
       "\n",
       "   jaccard_index_desc  lcs_title  lcs_desc  jscore_query_desc  \\\n",
       "0            0.000000          6        13           2.833333   \n",
       "1            0.000000          3         7           0.000000   \n",
       "2            0.012048          4         4           2.711111   \n",
       "\n",
       "   jscore_query_title  search_title_SW  search_desc_SW  NCD_query_title  \n",
       "0            0.866667              1.0             4.0         0.107077  \n",
       "1            0.000000              0.0             0.0         0.107077  \n",
       "2            0.000000              0.0             3.0         0.109091  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_similarity_features = modified_train[getSimilarityCols(all_num_features)]\n",
    "all_similarity_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:27:16.782699Z",
     "start_time": "2018-12-07T01:27:16.775996Z"
    }
   },
   "outputs": [],
   "source": [
    "count_cols, len_h_cols = getCountAndOtherCols(\n",
    "    all_similarity_features, all_num_features)\n",
    "all_count_features = modified_train[count_cols]\n",
    "len_entropy_features = modified_train[len_h_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:27:18.416454Z",
     "start_time": "2018-12-07T01:27:18.387952Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_numerical_subset, test_data_numerical_subset, train_target_numerical_subset, test_target_numerical_subset = train_test_split(all_num_features,\n",
    "                                                                                                                                        y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:41:41.205114Z",
     "start_time": "2018-12-06T03:41:23.099091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.24535714 2.49071429 2.64038095 2.60203571 2.58490476]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_transformed_rf_model.fit(\n",
    "    train_data_numerical_subset, train_target_numerical_subset)\n",
    "transformed_rf_predicted_numerical_subset = best_transformed_rf_model.predict(\n",
    "    test_data_numerical_subset)\n",
    "print(transformed_rf_predicted_numerical_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:50:20.686019Z",
     "start_time": "2018-12-06T03:50:03.047809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.22330935 2.45896043 2.65258993 2.58966427 2.46791367]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_rf_model.fit(train_data_numerical_subset, train_target_numerical_subset)\n",
    "rf_predicted_numerical_subset = best_rf_model.predict(\n",
    "    test_data_numerical_subset)\n",
    "print(rf_predicted_numerical_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:41:41.241209Z",
     "start_time": "2018-12-06T03:41:41.208502Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_similarity_subset, test_data_similarity_subset, train_target_similarity_subset, test_target_similarity_subset = train_test_split(all_similarity_features,\n",
    "                                                                                                                                            y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:41:56.375934Z",
     "start_time": "2018-12-06T03:41:41.244455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.11857143 2.27058673 2.1015     2.44828571 2.39452381]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_transformed_rf_model.fit(\n",
    "    train_data_similarity_subset, train_target_similarity_subset)\n",
    "transformed_rf_predicted_similarity_subset = best_transformed_rf_model.predict(\n",
    "    test_data_similarity_subset)\n",
    "print(transformed_rf_predicted_similarity_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:54:14.717152Z",
     "start_time": "2018-12-06T03:53:59.542835Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.02188849 2.34651079 2.10170264 2.58411871 2.42079137]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_rf_model.fit(train_data_similarity_subset, train_target_similarity_subset)\n",
    "rf_predicted_similarity_subset = best_rf_model.predict(\n",
    "    test_data_similarity_subset)\n",
    "print(rf_predicted_similarity_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:59:35.877474Z",
     "start_time": "2018-12-06T03:59:35.856394Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_count_subset, test_data_count_subset, train_target_count_subset, test_target_count_subset = train_test_split(all_count_features,\n",
    "                                                                                                                        y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:59:44.036371Z",
     "start_time": "2018-12-06T03:59:36.490203Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.62299464 2.61285714 2.37335278 2.22497168 2.24638285]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_transformed_rf_model.fit(\n",
    "    train_data_count_subset, train_target_count_subset)\n",
    "transformed_rf_predicted_count_subset = best_transformed_rf_model.predict(\n",
    "    test_data_count_subset)\n",
    "print(transformed_rf_predicted_count_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:00:34.417299Z",
     "start_time": "2018-12-06T04:00:26.533260Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.62617557 2.52879496 2.36541988 2.20950156 2.27060029]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_rf_model.fit(train_data_count_subset, train_target_count_subset)\n",
    "rf_predicted_count_subset = best_rf_model.predict(test_data_count_subset)\n",
    "print(rf_predicted_count_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:05:03.345733Z",
     "start_time": "2018-12-06T04:05:03.329956Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_len_entropy_subset, test_data_len_entropy_subset, train_target_len_entropy_subset, test_target_len_entropy_subset = train_test_split(len_entropy_features,\n",
    "                                                                                                                                                y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:08:03.419036Z",
     "start_time": "2018-12-06T04:07:46.615343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.54094286 2.59914286 2.36814286 2.59955051 2.06421429]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_transformed_rf_model.fit(\n",
    "    train_data_len_entropy_subset, train_target_len_entropy_subset)\n",
    "transformed_rf_predicted_len_entropy_subset = best_transformed_rf_model.predict(\n",
    "    test_data_len_entropy_subset)\n",
    "print(transformed_rf_predicted_len_entropy_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:08:46.975873Z",
     "start_time": "2018-12-06T04:08:30.610770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsandor/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.51521223 2.61115108 2.4346283  2.65931655 1.94434053]\n",
      "       relevance\n",
      "64115       2.33\n",
      "55409       2.33\n",
      "57088       2.00\n",
      "38621       3.00\n",
      "50452       2.33\n"
     ]
    }
   ],
   "source": [
    "best_rf_model.fit(train_data_len_entropy_subset,\n",
    "                  train_target_len_entropy_subset)\n",
    "rf_predicted_len_entropy_subset = best_rf_model.predict(\n",
    "    test_data_len_entropy_subset)\n",
    "print(rf_predicted_len_entropy_subset[:5])\n",
    "print(test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also considered stacking and tested out a few stacked models to see how much of an improvement, if any, there was over using individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_regression = StackedRegressor([RandomForestRegressor(n_estimators=140)], LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_regression.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = stacked_regression.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_regression_v2 = StackedRegressor([RandomForestRegressor(n_estimators=140, max_features='log2', max_depth=9, min_samples_split=5),\n",
    "                                       LinearRegression(),\n",
    "                                      GradientBoostingRegressor(loss='ls', learning_rate=0.4, n_estimators=150),\n",
    "                                      AdaBoostRegressor(n_estimators=6, loss='linear', learning_rate=0.25)], \n",
    "                                      RandomForestRegressor(n_estimators=140, max_features='log2', max_depth=9, min_samples_split=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_regression_v2.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions_v2 = stacked_regression_v2.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark was ~rank 1681 on the Kaggle leaderboard for this competition with an RMSE of .51049\n",
    "\n",
    "1st place had an RMSE of .43192\n",
    "\n",
    "https://www.kaggle.com/c/home-depot-product-search-relevance/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:15:44.993466Z",
     "start_time": "2018-12-19T09:15:44.631467Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-08495f9ba97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrmse_lin_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{rmse_lin_reg:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted' is not defined"
     ]
    }
   ],
   "source": [
    "rmse_lin_reg = sqrt(mean_squared_error(predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_lin_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:15:44.994586Z",
     "start_time": "2018-12-19T09:11:58.439Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_lin_reg_scaled = sqrt(mean_squared_error(scaled_linear_predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_lin_reg_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:15:44.996618Z",
     "start_time": "2018-12-19T09:11:59.176Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_tree = sqrt(mean_squared_error(tree_predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:15:44.998269Z",
     "start_time": "2018-12-19T09:11:59.772Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_alt_linear = sqrt(mean_squared_error(alt_linear_predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_alt_linear:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:15:45.000055Z",
     "start_time": "2018-12-19T09:12:00.344Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_rf = sqrt(mean_squared_error(rf_predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:16:27.570856Z",
     "start_time": "2018-12-19T09:16:27.566481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4658\n"
     ]
    }
   ],
   "source": [
    "rmse_transformed_rf = sqrt(mean_squared_error(\n",
    "    transformed_rf_predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_transformed_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:15:45.004503Z",
     "start_time": "2018-12-19T09:12:04.917Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_gb = sqrt(mean_squared_error(\n",
    "    gb_predicted, test_target))\n",
    "\n",
    "print(f\"{rmse_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rmse = sqrt(mean_squared_error(xgb_predictions, test_target))\n",
    "\n",
    "print(f\"{xgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_stacked = sqrt(mean_squared_error(\n",
    "    stacked_predictions, test_target))\n",
    "\n",
    "print(f\"{rmse_stacked:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_stacked_v2 = sqrt(mean_squared_error(\n",
    "    stacked_predictions_v2, test_target))\n",
    "\n",
    "print(f\"{rmse_stacked:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Subset RMSE - Random Forest after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:02:42.666595Z",
     "start_time": "2018-12-06T03:02:42.662121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5992\n"
     ]
    }
   ],
   "source": [
    "rmse_transformed_rf_numerical_subset = sqrt(mean_squared_error(\n",
    "    transformed_rf_predicted_numerical_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_transformed_rf_numerical_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:42:59.364815Z",
     "start_time": "2018-12-06T03:42:59.359594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5953\n"
     ]
    }
   ],
   "source": [
    "rmse_transformed_rf_predicted_similarity_subset = sqrt(\n",
    "    mean_squared_error(transformed_rf_predicted_similarity_subset, test_target))\n",
    "\n",
    "print(f\"{transformed_rf_predicted_similarity_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:02:47.758708Z",
     "start_time": "2018-12-06T04:02:47.753709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6039\n"
     ]
    }
   ],
   "source": [
    "rmse_transformed_rf_count_subset = sqrt(mean_squared_error(\n",
    "    transformed_rf_predicted_count_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_transformed_rf_count_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:12:32.832888Z",
     "start_time": "2018-12-06T04:12:32.828041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5788\n"
     ]
    }
   ],
   "source": [
    "rmse_transformed_rf_len_entropy_subset = sqrt(mean_squared_error(\n",
    "    transformed_rf_predicted_len_entropy_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_transformed_rf_len_entropy_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Subset RMSE - Regular Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:51:04.842927Z",
     "start_time": "2018-12-06T03:51:04.838276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5988\n"
     ]
    }
   ],
   "source": [
    "rmse_rf_numerical_subset = sqrt(mean_squared_error(\n",
    "    rf_predicted_numerical_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_rf_numerical_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T03:54:27.244229Z",
     "start_time": "2018-12-06T03:54:27.237178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5950\n"
     ]
    }
   ],
   "source": [
    "rmse_rf_similarity_subset = sqrt(mean_squared_error(\n",
    "    rf_predicted_similarity_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_rf_similarity_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:03:15.490850Z",
     "start_time": "2018-12-06T04:03:15.482726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6036\n"
     ]
    }
   ],
   "source": [
    "rmse_rf_count_subset = sqrt(mean_squared_error(\n",
    "    rf_predicted_count_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_rf_count_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T04:13:20.589394Z",
     "start_time": "2018-12-06T04:13:20.583976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5786\n"
     ]
    }
   ],
   "source": [
    "rmse_rf_len_entropy_subset = sqrt(mean_squared_error(\n",
    "    rf_predicted_len_entropy_subset, test_target))\n",
    "\n",
    "print(f\"{rmse_rf_len_entropy_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we performed the same transformation used upon the training data on the test data. If you would prefer not to go through the transformation process, there is a cell below where you can load the saved results of the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:09:23.546673Z",
     "start_time": "2018-12-21T08:09:23.226894Z"
    }
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('Data/product_descriptions.csv')\n",
    "test = pd.read_csv('Data/test.csv', encoding='ISO-8859-1')\n",
    "attributes = pd.read_csv('Data/attributes.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:28:37.581856Z",
     "start_time": "2018-12-14T22:25:50.986628Z"
    }
   },
   "outputs": [],
   "source": [
    "# BEWARE: this takes ~1 min to run\n",
    "attrib_per_product_test = attrib_stack(attributes, 'Data/attrib_per_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:29:01.946156Z",
     "start_time": "2018-12-14T22:28:37.584222Z"
    }
   },
   "outputs": [],
   "source": [
    "test, attrib_per_product_test = join_attrib(test, attrib_per_product_test)\n",
    "test = search_term_in_attrib(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:33:41.032591Z",
     "start_time": "2018-12-14T22:29:01.947795Z"
    }
   },
   "outputs": [],
   "source": [
    "test = color_df(attributes, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:40:48.336896Z",
     "start_time": "2018-12-14T22:40:48.334588Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_test = test.set_index('product_uid').join(\n",
    "        products.set_index('product_uid'))\n",
    "modified_test = modified_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:41:47.411463Z",
     "start_time": "2018-12-14T22:40:57.083041Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_test['total_description'] = modified_test['product_title'] + \\\n",
    "        modified_test['product_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:58:41.213827Z",
     "start_time": "2018-12-14T22:58:05.180653Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_file = 'Data/glove.6B.300d.txt'\n",
    "glove_dic = make_dictionary(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T22:58:45.966851Z",
     "start_time": "2018-12-14T22:58:41.218071Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_test = test.set_index('product_uid').join(\n",
    "        products.set_index('product_uid'))\n",
    "modified_test = modified_test.reset_index()\n",
    "modified_test = create_cleaned_terms_col(modified_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = modified_test[['product_title', 'search_term',\n",
    "                        'name', 'value', 'search_term_split', 'search_term_in_attrib',\n",
    "                        'product_description', 'product_uid', 'total_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, we then use our best model thus far to perform the transformation upon the test set to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:34:35.289641Z",
     "start_time": "2018-12-21T08:34:26.657710Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_test = pd.read_csv('./Data/feature_engineered_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:34:37.508362Z",
     "start_time": "2018-12-21T08:34:37.470418Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = modified_test[['clean_length', 'title_length',\n",
    "                          'desc_length', 'clean_terms_in_title',\n",
    "                          'clean_terms_in_desc', \n",
    "                          'min_levenstein_dist_title', 'min_levenstein_dist_brand',\n",
    "                          'stemmed_terms_in_title', 'stemmed_terms_in_desc',\n",
    "                          'lemmatized_terms_in_title', 'lemmatized_terms_in_desc',\n",
    "                          'neighbours_in_title', 'neighbours_in_desc', 'search_terms_entropy',\n",
    "                          'title_entropy', 'jaccard_index_title', 'jaccard_index_desc', 'lcs_title',\n",
    "                          'lcs_desc', 'jscore_query_desc', 'jscore_query_title', 'search_title_SW',\n",
    "                          'search_desc_SW', 'NCD_query_title', 'num_words_in_description', 'num_stop_words',\n",
    "                          'num_search_words', 'tfidf_search_common', 'num_attrib']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T19:56:36.388357Z",
     "start_time": "2018-12-21T19:51:46.575818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.4829405264940038, {'rf_reg__max_depth': 9, 'rf_reg__max_features': 'log2', 'rf_reg__min_samples_split': 5, 'rf_reg__n_estimators': 143}, Pipeline(memory=None,\n",
      "     steps=[('rf_reg', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]))]\n"
     ]
    }
   ],
   "source": [
    "# BEWARE : this takes ~17m to run\n",
    "rf_models = [Pipeline([('rf_reg', RandomForestRegressor())])]\n",
    "\n",
    "grid_params_rf = [{'rf_reg__n_estimators': [143],\n",
    "                   'rf_reg__max_features': ['log2'],\n",
    "                   'rf_reg__max_depth': [9],\n",
    "                   'rf_reg__min_samples_split': [5]}]\n",
    "best_models_rf = []\n",
    "for model in zip(rf_models, grid_params_rf):\n",
    "    gs = GridSearchCV(estimator=model[0],\n",
    "                      param_grid=model[1],\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      cv=5)\n",
    "    if type(y_train) != np.ndarray:\n",
    "        y = y_train.values.ravel()\n",
    "        y_train = np.array(y).astype(float)\n",
    "    gs.fit(X_train, y_train.ravel())\n",
    "    best_models_rf.append(\n",
    "        (sqrt(-1 * gs.best_score_), gs.best_params_, model[0]))\n",
    "\n",
    "print(best_models_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T19:56:36.393868Z",
     "start_time": "2018-12-21T19:56:36.390613Z"
    }
   },
   "outputs": [],
   "source": [
    "best_rf_model = change_best_params_keys(best_models_rf, \n",
    "                                        ['rf_reg__n_estimators', 'rf_reg__max_features', \n",
    "                                         'rf_reg__max_depth', 'rf_reg__min_samples_split'],\n",
    "                                        ['n_estimators', 'max_features', 'max_depth', 'min_samples_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T19:56:47.683982Z",
     "start_time": "2018-12-21T19:56:36.396344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.94516864 1.97368064 2.14306728 2.52168675 2.45696723]\n"
     ]
    }
   ],
   "source": [
    "best_rf_model = best_rf_model[2].steps[0][1].__class__(\n",
    "    **best_rf_model[1])\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "rf_predicted = best_rf_model.predict(X_test)\n",
    "print(rf_predicted[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T21:25:12.653243Z",
     "start_time": "2018-12-18T21:25:12.650248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166693,)\n"
     ]
    }
   ],
   "source": [
    "print(rf_predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T19:58:04.140907Z",
     "start_time": "2018-12-21T19:58:01.846812Z"
    }
   },
   "outputs": [],
   "source": [
    "output = 'id,relevance\\n'\n",
    "predictions = '\\n'.join([str(test['id'][idx]) + ',' +str(prediction) for idx, prediction in enumerate(rf_predicted)])\n",
    "output += predictions\n",
    "    \n",
    "prediction_file = open('home_depot_search_relevancy_test_predictions.csv', 'w')\n",
    "prediction_file.write(output)\n",
    "prediction_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
